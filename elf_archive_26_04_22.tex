%%
%% This is file `team-eagle-lazer-fang.tex',
%%
%% A modified copy of:
%%
%% sample-authordraft.text
%% 
%% IMPORTANT NOTICE:
%% 
%% For the copyright see the source file.
%% 
%% Any modified versions of this file must be renamed
%% with new filenames distinct from sample-manuscript.tex.
%% 
%% For distribution of the original source see the terms
%% for copying and modification in the file samples.dtx.
%% 
%% This generated file may be distributed as long as the
%% original source files, as listed above, are part of the
%% same distribution. (The sources need not necessarily be
%% in the same archive or directory.)
%%
%% The first command in your LaTeX source must be the \documentclass command.
%%%% Small single column format, used for CIE, CSUR, DTRAP, JACM, JDIQ, JEA, JERIC, JETC, PACMCGIT, TAAS, TACCESS, TACO, TALG, TALLIP (formerly TALIP), TCPS, TDSCI, TEAC, TECS, TELO, THRI, TIIS, TIOT, TISSEC, TIST, TKDD, TMIS, TOCE, TOCHI, TOCL, TOCS, TOCT, TODAES, TODS, TOIS, TOIT, TOMACS, TOMM (formerly TOMCCAP), TOMPECS, TOMS, TOPC, TOPLAS, TOPS, TOS, TOSEM, TOSN, TQC, TRETS, TSAS, TSC, TSLP, TWEB.
% \documentclass[acmsmall]{acmart}

%%%% Large single column format, used for IMWUT, JOCCH, PACMPL, POMACS, TAP, PACMHCI
% \documentclass[acmlarge,screen]{acmart}

%%%% Large double column format, used for TOG
% \documentclass[acmtog, authorversion]{acmart}

%%%% Generic manuscript mode, required for submission
%%%% and peer review
\documentclass[manuscript,screen,review]{acmart}

\usepackage{tikz}
\usetikzlibrary{shapes.geometric, arrows, patterns}
\usepackage{graphicx}
\usepackage{subcaption}
% \usepackage{draftwatermark}
% \SetWatermarkText{\includegraphics[width = 0.25\textwidth, opacity = 0.1]{graphics/Untitled-1.png}}
%% Fonts used in the template cannot be substituted; margin 
%% adjustments are not allowed.
%%
%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    \normalfont B\kern-0.5em{\scshape i\kern-0.25em b}\kern-0.8em\TeX}}}

%% Rights management information.  This information is sent to you
%% when you complete the rights form.  These commands have SAMPLE
%% values in them; it is your responsibility as an author to replace
%% the commands and values with those provided to you when you
%% complete the rights form.
% \setcopyright{acmcopyright}
% \copyrightyear{2018}
% \acmYear{2018}
% \acmDOI{XXXXXXX.XXXXXXX}

% %% These commands are for a PROCEEDINGS abstract or paper.
% \acmConference[Conference acronym 'XX]{Make sure to enter the correct
%   conference title from your rights confirmation emai}{June 03--05,
%   2018}{Woodstock, NY}
% %
% %  Uncomment \acmBooktitle if th title of the proceedings is different
% %  from ``Proceedings of ...''!
% %
% \acmBooktitle{Woodstock '18: ACM Symposium on Neural Gaze Detection,
%  June 03--05, 2018, Woodstock, NY} 
% \acmPrice{15.00}
% \acmISBN{978-1-4503-XXXX-X/18/06}


%%
%% Submission ID.
%% Use this when submitting an article to a sponsored event. You'll
%% receive a unique submission ID from the organizers
%% of the event, and this ID should be used as the parameter to this command.
%%\acmSubmissionID{123-A56-BU3}

%%
%% The majority of ACM publications use numbered citations and
%% references.  The command \citestyle{authoryear} switches to the
%% "author year" style.
%%
%% If you are preparing content for an event
%% sponsored by ACM SIGGRAPH, you must use the "author year" style of
%% citations and references.
%% Uncommenting
%% the next command will enable that style.
%%\citestyle{acmauthoryear}

%%
%% end of the preamble, start of the body of the document source.
\begin{document}

%%
%% The "title" command has an optional parameter,
%% allowing the author to define a "short title" to be used in page headers.
\title{Trustworthiness of AI decision making tools in high-risk applications.}

%%
%% The "author" command and its associated commands are used to define
%% the authors and their affiliations.
%% Of note is the shared affiliation of the first two authors, and the
%% "authornote" and "authornotemark" commands
%% used to denote shared contribution to the research.
\author{Alex Davies}
\affiliation{%
  \institution{Interactive AI CDT, University of Bristol}
  \country{UK}
}
\author{Daniel Collins}
\affiliation{%
  \institution{Interactive AI CDT, University of Bristol}
  \country{UK}
}
\author{Isabella Degen}
\affiliation{%
  \institution{Interactive AI CDT, University of Bristol}
  \country{UK}
}
\author{Jonathan Erskine}
\affiliation{%
  \institution{Interactive AI CDT, University of Bristol}
  \country{UK}
}
\author{Matt Clifford}
\affiliation{%
  \institution{Interactive AI CDT, University of Bristol}
  \country{UK}
}
\author{Ronja Spaniel}
\affiliation{%
  \institution{Interactive AI CDT, University of Bristol}
  \country{UK}
}

%%
%% By default, the full list of authors will be used in the page
%% headers. Often, this list is too long, and will overlap
%% other information printed in the page headers. This command allows
%% the author to define a more concise list
%% of authors' names for this purpose.
\renewcommand{\shortauthors}{Team Eagle Lazer Fang, et al.}

%%
%% The abstract is a short summary of the work to be presented in the
%% article.
\begin{abstract}
Abstract
 Include motivation and Contribution
 what's our contribution - first sentence
 why are we doing this research -> one sentence
 mention survey of x people and focus group -> one sentence on what we did
 what have we done - why is it new - how have we done it - what are our findings (short)
 
 
 
\end{abstract}

%%
%% The code below is generated by the tool at http://dl.acm.org/ccs.cfm.
%% Please copy and paste the code instead of the example below.
%%
% \begin{CCSXML}
% <ccs2012>
%  <concept>
%   <concept_id>10010520.10010553.10010562</concept_id>
%   <concept_desc>Computer systems organization~Embedded systems</concept_desc>
%   <concept_significance>500</concept_significance>
%  </concept>
%  <concept>
%   <concept_id>10010520.10010575.10010755</concept_id>
%   <concept_desc>Computer systems organization~Redundancy</concept_desc>
%   <concept_significance>300</concept_significance>
%  </concept>
%  <concept>
%   <concept_id>10010520.10010553.10010554</concept_id>
%   <concept_desc>Computer systems organization~Robotics</concept_desc>
%   <concept_significance>100</concept_significance>
%  </concept>
%  <concept>
%   <concept_id>10003033.10003083.10003095</concept_id>
%   <concept_desc>Networks~Network reliability</concept_desc>
%   <concept_significance>100</concept_significance>
%  </concept>
% </ccs2012>
% \end{CCSXML}

% \ccsdesc[500]{Computer systems organization~Embedded systems}
% \ccsdesc[300]{Computer systems organization~Redundancy}
% \ccsdesc{Computer systems organization~Robotics}
% \ccsdesc[100]{Networks~Network reliability}

%%
%% Keywords. The author(s) should pick words that accurately describe
%% the work being presented. Separate the keywords with commas.
\keywords{XAI, Explainable AI, Trustworthy AI, Interpretable AI, AI, Computer Automation, AI in high risk}

%%
%% This command processes the author and affiliation and title
%% information and builds the first part of the formatted document.
\maketitle

\newpage
\section{Introduction}\label{sec:introduction}
%%%%%%%%%%%
% Keeping notes from previous list making
%%%%%%%%%%%
%Motivate the research
%give an overview of relevant background literature
%describe the research question.
%Possibly signpost the findings
% notes on how to writing
% go straight to the point
% don't make it so general that it could be used for an other paper
% don't slash others works
% include problem / solution

%Initial structure:
%\begin{itemize}
%    \item AI used in many use applications (give examples of high risk etc) - we focused on healthcare
%    \item For successful integration of AI, users and public need to trust it. 
%    \item Law came into that decisions for banks etc needed to be explained to the user, the right to explanation, e.g GDPR %https://www.turing.ac.uk/research/impact-stories/a-right-to-explanation
%    \item this brought about interest in XAI (Why XAI has been seen as important (Issues with this - black box models, not always human able to intervene/oversee))
%    \item Lots of work has been done into XAI techniques to give `trust' in AI.
%    \item XAI examples to show off the amount of work been done  - survey papers and specific examples? LIME? FATF?
%    \item Issues with XAI - critics (to give some doubt over XAI actually giving trust and telling us how AI works)
%    \item Is the current work in XAI just useful for machine learning practitioners? 
%    \item What is best for the users/ what do users actually want? Has the explanation law confused us of what users actually want/need?
%    \item We want to explore what trust actually is in relation to AI. Is it XAI? Is it performance? Is it counterfactual results? (are they more intuitive/what people would like to know)?
%    \item There has been the topic of trust between humans and machines before (1987 paper). Which indicates that there needs to be trust not just performance.
%    \item There is no current user study of how XAI changes trust depending on: demographic (AI expert, high stakes decision maker, general public) or the level of explainablity. 
%    % \item medical domain - paper Alex found on simple AI in a medical context out-performing doctors
%\end{itemize}
%Ann's suggestion of structure:
%\begin{itemize}
%    \item Context (how is the world now): Messy, AI and Explainable AI, 
%    \item the problem (drama): loads of research, not many survey on what's needed on trust, does transparency and explainable creates more or less trust
%    \item related work and how it doesn't cover this: GDPR has created a flood of research into Explainable AI, so it's not been questioned much on the motivations. Surveys on trust in generic AI systems. Surveys of perspectives on generic AI system for healthcare work groups.
%    \item our solution, method and results: survey and focus group to find out if trust increases with transparency and if experience with AI or high stakes decision making matters
%    \item what new worlds does our research do: design with all demographics in mind, different branches of explainability. Present information that can be used to design better AI systems for healthcare needs. Encourage more user driven explainable AI with the demographics we find
%\end{itemize}

% Isabella: I've kept the bullet points for the text below in my google doc if needed
%%%%%%%%%%%%%%%%


% Why Context - how is the world now
AI is being deployed in a variety of different domains, including high-risk application areas where algorithms are potentially making life-altering decisions; from insurance to criminal justice to healthcare systems \cite{Bohr2020}, \cite{Chen2018}. It is widely believed that new laws like the EU General Data Protection Regulation (GDPR) mandate a ‘right to explanation’ as a mechanism to enhance the accountability and transparency of automated decision-making \cite{Wachter2017}. While that right does not explicitly exist \cite{Wachter2017}, the approval of the GDPR in 2016 created an explosion of research in Explainable AI (XAI) and closely related research areas such as: Transparent AI, Interpretable AI and Trustworthy AI. The desired outcome of these research areas is to create responsible and ethical AI systems that are fair and allow for accountability \cite{Mohseni2021}, often referred to as Responsible AI. This has led to a surge of new methods that explain the decisions of different AI algorithms, see \cite{Linardatos2021} and \cite{Guidotti2018a} for an overview. There are endeavours to standardise and asses explanations \cite{Gilpin2019}; as well as countless literature surveys aiming to create an unified and standardised taxonomy and definitions for explainable, transparent, interpretable and trustworthy AI \cite{Schwalbe2021} and \cite{Arrieta2020}.\\\\

%Why new - The problem (drama)
The huge increase in the popularity of explainable, interpretable and trustworthy AI has given rise to critical views. The motivations behind explainable AI, the quality \cite{Doshi-Velez2017} and usefulness of the explanations given \cite{Miller2019} as well as whether explainable AI achieves its objective of higher trust in AI \cite{Lipton2018} have been studied. Research into explainable AI recognises that there are many different stakeholders of AI systems such as: developers, end-users and lawmakers. The needs of each of these stakeholders vary greatly. For the developers of AI systems the explanations are useful for evaluating, testing and debugging purposes. They have a deep and technical understanding of the concepts and mathematics behind the algorithms and are therefore able to interpret mathematical explanations of those systems. These are the people who are creating new explainability methods and we refer to them as AI experts \cite{Carvalho2019}. The end users of the AI systems are people who use these systems in their work e.g clinicians. They are domain experts in the problem that the AI system solves. These end users are interested in explanations to help them assess the model’s decision and decide on the actions to be taken. Examples of research into how well their needs are served with interpretable and explainable AI can be found in \cite{Goldstein2021}, \cite{Liao2020}, \cite{Salimiparsa2021}. However, there is a lack of research into whether people who are affected by the decisions of an AI system, for example patients that are being diagnosed by an AI system, are asking for more transparent and explainable AI systems.\\\\

%more related work
A study surveyed 922 women’s attitude toward AI in mammography screening. They found that they preferred a combination of radiologist and AI system \cite{Ongena2021}. Another study surveyed 216 fracture patients on their perceptions towards AI used to assist in interpretation of their radiographs and found that participant’s held the clinician’s assessment in the highest regard \cite{York2020}. A further study found that patients are apprehensive about the use of AI \cite{Richardson2021} and highlighted the need for early patient involvement in the AI systems being used in healthcare. These studies did not investigate the role that transparency and performance of the AI system played in the patients attitudes.\\\\

% more related work and how it doesn't cover this: Surveys on trust in generic AI systems. Surveys of perspectives on generic AI systems for healthcare work groups.
There is no current user study of how XAI changes trust depending on: demographic (AI expert, high stakes decision maker, general public) nor the level of explainability.\\  

%What & How: our solution, method and results: survey and focus group to find out if trust increases with transparency and if experience with AI or high stakes decision making matters
In this study we are focusing on the opinions of people who are affected by the decision of an AI system. We created a survey of $n=277$ people who were asked to imagine being diagnosed in a hospital whether to have treatment for COVID or not in four different scenarios:

\begin{enumerate}
    \item A doctor makes the diagnosis.
    \item A doctor makes the diagnosis together with an interpretable AI system (a so called glass box system).
    \item A doctor makes the diagnosis together with an AI system that’s not interpretable (a so called black box system).
    \item A black box system makes the diagnosis by itself.
\end{enumerate}

For each of these scenarios the participants were told how often the doctor and AI system make the correct diagnosis (the performance) and how much the doctor knew about why the AI system made its decision (the interpretability of the system). The participants were asked to express how much they would trust to be diagnosed by that setup by selecting a number from 1-10. We furthermore collected demographic information about the participants, namely if they are working in healthcare or not, how much experience and knowledge they have about AI as well as how frequently they are making high-stakes decisions (life-altering decisions). The scenarios were designed in such a way that of the patients who have Covid the doctor and glass box both correctly diagnose 85 out of 100 while the black box had a significantly better performance of 95 correct diagnoses out of 100. This is an equivalent to a "true positive rate" in information science, or "sensitivity" in medicine, which are accepted measures of performance in both fields.

We analysed the responses with regards to which scenario was trusted the most respectively least and how this changed depending on the participants demographic. We found that overall scenario 4 (the black box on its own) was the least trusted scenario and scenario 2 (the doctor working together with the glass box) was the most trusted scenario. Scenario 1 (the doctor on their own) surprisingly was the second most frequent lowest ranked scenario, while it was at the same time the second most frequent highest ranked scenario. Scenario 3 (the doctor working with the black box) gained significantly more trust than scenario 4 (the black box on its own). The three demographic aspects did not change this other than for the participants that reported making frequent high stakes decisions, they gave the doctor by themselves the least trust. Furthermore, participants with medium and high AI expertise trust scenario 3 (the doctor with the black box) more. Participants who work in healthcare either trust the doctor by itself a lot or not at all, while participants who don’t work in healthcare trust the black box by itself significantly less.

We followed the survey up with a focus group of 6 people with demographics <?> to discuss and reflect further on their thoughts about AI and the need for interpretability and explanations as well as exploring if black box models will ever be accepted. We found that the dislike of the black box has less to do with the lack of interpretablity, but more to do with the lack of empathy that a machine can give compared to a human when diagnosing someone with a potential critical illness. However, the focus group rated the black box higher than the survey participant did under the conditions that it had high performance and that the patient had full disclosure of a black box being used or that it was used in a low-stakes application area.


%Why relevant: what new worlds does our research do: design with all demographics in mind, different branches of explainability. Present information that can be used to design better AI systems for healthcare needs. Encourage more user driven explainable AI with the demographics we find
<todo> We believe that for an AI system to be adapted in a critical decision scenario, such as diagnosis in health care, it is not sufficient to just make the system transparent and explainable. It is far more important to complete the user experience and give an understanding that the AI system is only a part of that experience. We believe that more research is needed to understand this area fully.\\
The current techniques are useful for the operators of the systems, however, the law is to protect the end user that is affected by an AI having been used. It is clear that the needs for these groups are different and more research needs to be done to consider the end users of those systems and what their needs are and how they differ from the operators.\\


\section{Related Work}\label{sec:related-work}
%Outline:
%
% What is Explainable AI
% What is interpretable AI
% What is transparency AI
% What is trustwortiness in AI
% Why is Explainable AI needed
% How is XAI different in Healtcare
% Shortfalls and critique of XAI, how XAI is not needed, include critics of HCI, difficulties of too many stakehoders
%
% Where do the methods go? Do they fit here

\subsection{An overview of transparent, interpretable and explainable AI}\label{subsec:an-overview-of-transparent-interpretable-and-explainable-ai}
The field of explainable AI (XAI) experienced a huge surge since 2016 an there is no shortage of surveys of the research.

The literature speaks of black and glass boxes (also called white box) system.
Black box models are complex systems that are not transparent.
It is not clear to a human how and why the system makes a decision.
On the other hand glass boxes (frequently also called white box) models are systems that are interpretable and transparent by default.
Explanations can be seen as adding a form of post-hoc interpretability to an AI system and with that make AI models more transparent with regards to how they make decisions.
There's a wide variety of terminologies, motivations, approaches and evaluation criteria for XAI methods.
Many researchers have create surveys of these methods and attempted to create a unified taxonomy of these methods\cite{Linardatos2021} and\cite{Schwalbe2021}.
However, despite the vast body of knowledge developed around the concept of explainability, there is not a general consensus among scholars on how an explanation should be defined, and how its validity and reliability assessed\cite{Vilone2021}.

There are also many surveys that focus on summarizing the available explanation methods such as\cite{Vilone2020},
a field guide of explainable AI for Deep Learning\cite{Ras2020}, a survey of methods for explaining black box models\cite{Guidotti2018a} \&\cite{Adadi2018},
a survey of post-hoc explanation by example methods\cite{Keane2019}, contrastive explanations\cite{Miller2021},
explanations using counterfactuals\cite{Verma2020} \&\cite{Keane2020}, rule based explanations\cite{Guidotti2018} as well
as conversational explanations\cite{Sokol2018}.

Note that this recent wave of explanations of automated decision making is not the first wave.
An earlier wave happened around automated decision making in control systems as well as AI in the late 80ies see\cite{Chandrasekaran1989}.

\subsection{Why Explainable AI is needed}\label{subsec:why-explainable-ai-is-needed}
Gunning states "Explainability is essential for users to effectively understand, trust, and manage powerful artificial intelligence applications."\cite{Gunning2019}

Bansal and co studied if explanations increase the performance of the human-AI team and found that this was not the case.
Instead they found that the explanations increased the chance that the human will accept the AI's recommendation regardless of
the correctness thereof.
Their findings suggest that explanations increase trust in AI\cite{Bansal2020}.

Weld and co found that to trust an AI system's behavior, it must be intelligible, either by using inherently interpretable models or by developing new methods for explaining and controlling otherwise overwhelmingly complex decisions using local approximation, vocabulary alignment, and interactive explanation\cite{Weld2019}.


\subsection{Shortfalls and Critics of current XAI}\label{subsec:shortfalls-and-critics-of-current-xai}
XAI has also received a fair amount of critique.
From questioning if explainability and interpretability can be achieved to the vague definitions of the terms used\cite{Lipton2018}.

Researchers have also been starting to investigate if supposedly interpretable models achieve their intended effects, such as making people more closely follow a model's predictions when it is beneficial for them to do so or enabling them to detect when a model has made a mistake.
They however did not find that participants more closely followed its predictions\cite{Goldstein2021}.

Ehsan and co argue that explanations need to be more human centered.
Building on their case study they stress that more focus on the human is needed and that technical advancements and the understanding of human factors co-evolve together.\cite{Ehsan2020}

However, exactly what kinds of explanation are truly human-interpretable remains poorly understood.
This work advances our understanding of what makes explanations interpretable in the specific context of verification.
In general, greater complexity results in higher response times and lower satisfaction\cite{Narayanan2018}

Felzmann and co explore what the requirement of transparency in the GDPR entails for AI and automated decision-making systems integrating integrating legal, social, and ethical aspects.
They highlight how human–computer interaction and human-robot interaction literature do not provide clear results with respect to the benefits of transparency for users of artificial intelligence technologies
and propose a relational concept of transparency. 
They suggest attention needs to be paid to factors that make transparency meaningful and trustworthy in the users’ eyes.\cite{Felzmann2019}

Markus and co find that evidence of the usefulness of explainability is still lacking in practice and recognize that complementary measures might be needed to create trustworthy AI\cite{Markus2021}


\subsection{Connecting Human Computer Interaction and XAI}\label{subsec:human-computer-interaction-focused-xai}
Mohsensi and co's research created a framework to support diverse design goals and evaluation methods in XAI research\cite{Mohseni2021}

Wang and co created a framework based on decision making and reasoning theory for AI explanation techniques.
They aim to help developers build human-centric explainable AI-based systems with targeted XAI features\cite{Wang2019}

Amershi and co proposed and evaluated 18 generally applicable design guidelines for human-AI interaction that they hope will result in
better, more human-centric AI-infused systems. 
These guidelines include principles relevant to our study such as "Make clear why the system did what it did" as well as "Make clear how well the system can do what it can
do"\cite{Amershi2019}.

Researchers from the HCI communities are interested to empower people and to ensure that new and powerful AI technologies are designed with intelligibility from the ground up.
They highlight that while researchers in the ML and AI communities are working on making their algorithms explainable, their focus is not on usable, practical and effective transparency that works for and
benefits people.
And stress that explainable systems can only be created through interdisciplinary collaborations, where expertise from different
fields (e.g., machine learning, cognitive psychology, human computer interaction) is combined and concepts and techniques are further developed from multiple perspectives to
move research forward.\cite{Abdul2018}

\subsection{Trust in automated decision making}\label{subsec:trust}
Trust has long been recognised as an important factor in facilitating Human Computer Interaction\cite{Lee2008}.
In the social sciences trust is studied between humans.
It's that body of work that the study of trust of humans in machines relies on.
Long before the recent AI summer, research has been done into understanding the nature of trust between humans and machines
and how its important to consider what grows and diminishes trust when designing decision aids to avoid the machine's
decision being rejected regardless from how sophisticated or "intelligent" it is.\cite{Muir1987}
In her research Muir also discusses that a machine needs to be designed in such a way that the user can perceive it's trustworthiness\cite{Muir1987}.
Building AI systems that are interpretable and or can explain their decision is perhaps away for a user to be able to perceive
the algorithms trustworthiness.\\
Despite extensive research, trust remains hard to define and there are contradicting findings and suggestions.
Many researchers agree that the performance of the decision making machine alone is not sufficient to build trust.
Muir suggests that while the machine needs to be designed well the human using it might need to recalibrate their
trust in the machine's decision too\cite{Muir1987}.
Hoff and co identify three layers of variability in human–automation trust: dispositional trust, situational trust, and learned trust.
Additionally they identify also environmental conditions that can affect the strength of the relationship between trust and reliance\cite{Hoff2015}.
On the other hand Hancock and co found that a robot's performance and attributes were the largest contributors to the development of trust in human robot interaction and that environmental factors played only a moderate role.
They also state that there is little evidence for effects of human-related factors\cite{Hancock2011}.\\
More recently researcher have specifically looked at trust and AI.
Toreini and co and they to rely on trust as a notion within the social sciences.
They show that Fair, Explainable, Auditable and Safe technologies need to be considered in the
various stages of an AI system's life cycle and with that form a Chain of Trust required for trustworthy machinelearning\cite{Toreini2020}.
Chen argues that for trust to exist a set of conditions need to be satisfied.
He suggests that trust needs to be engineered either through involving a certain reliance on a set of physiognomic biases or by relying on a set of esteem-seeking psychological mechanisms.
While this "Trust-Engineering" will not fully achieve trustworthy AI he argues it will ensure that reliance-without-trust can be avoided\cite{Chen2021}.
Finally Ferrario and co developed a multi-layer model of trust for human AI interaction\cite{Ferrario2020}.

\subsection{AI and XAI in healthcare}\label{subsec:ai-in-healthcare}
For this study we focused on diagnosis decision in a healthcare settings.There's evidence that public disagreement/agreement with using AI varied depending on the application area\cite{Ikkatai2022}.
Holzinger and co argue that research in explainable-AI helps to facilitate the implementation of AI/ML in the medical domain, and specifically helps to facilitate transparency and trust\cite{Holzinger2017}.
As we've mentioned earlier explanation are useful to find out if the system has learned something about medical pathology or not.
This has been demonstrated by DeGrave, they found a deep learning system used to detect COVID-19 from chest radiographs relied on confounding factors rather than medical pathology, creating an alarming situation in which the system appear accurate despite not having learned anything about medical pathology.
They conclude that their findings demonstrate that explainable AI should be seen as a prerequisite to clinical deployment of machine-learning healthcare models\cite{DeGrave2021}.

In their literature survey Nazar and co make the point of how explainable AI connects AI with Human Computer Interaction (HCI).
They also stress that more research is needed into AI and XAI in healthcare.
The hope is that explanations can be compared and validated with clinical knowledge and through that new
clinical knowledge can be created or AI decisions corrected\cite{Nazar2021}.

There is also research showing that AI can be used for diagnosing Covid.
Guefrechi and co show transfer learning proves to be effective, showing strong performance and easy-to-deploy COVID-19 detection methods.
This enables automatizing the process of analyzing X-ray images with high accuracy and it can also be used in cases where the materials and RT-PCR tests are limited\cite{Guefrechi2021}.

The needs depending on the stakeholders in healthcare also change.
Medical physicist are the end-users.
Diaz and co investigated their current perceptions, practices and education needs and found that while the value of AI is recognised by medical physicists, more training of the operators of the system is needed.
They also conclude that medical physicists should be more included in AI projects\cite{Diaz2021}.

%we could move these papers to the discussion:
The patients' view on the implementation of artificial intelligence in radiology is still mainly unexplored territory.
Ongena and co found the following 5 factors mattered the most: (1) distrust and accountability (overall, patients were moderately negative on this subject), (2) procedural knowledge (patients generally indicated the need for their active engagement), (3) personal interaction (overall, patients preferred personal interaction), (4) efficiency (overall, patients were ambiguous on this subject), and (5) being informed (overall, scores on these items were not outspoken within this factor)\cite{Ongena2020}.

%Is accuracy important?
Yin and co found that people's trust in a model is affected by both its stated accuracy and its observed accuracy, and that the effect of stated accuracy can change depending on the observed accuracy\cite{Yin2019}.



\section{Methodology}\label{sec:methodology}
% Describe how we did user research
% How are we going to evaluate our findings

%The aim of this study is to investigate how the level of transparency that is provided by an AI tool influences the level of trust placed in it when used in high-stakes decision-making. Transparency of an AI tool refers to how well a human can understand why an AI tool has come to the decision that it has made. A high-stakes decision is where someone may be significantly negatively impacted by the result of a decision, for example self-driving cars, medical and legal decisions

The aim of this study is to investigate how the level of transparency that is provided by an AI tool influences a users' level of trust when applied in the context of high-stakes decision-making. In this context, transparency refers to a characteristic of the AI tool being used, which implies how easily the model outputs can be justified by a user when considering the model inputs. A high-stakes decision is one where someone may be significantly negatively impacted by the outcome of the decision e.g. self-driving cars, medical and legal decisions.\\

To address the aims of this study, we conducted a two-part investigation. First, we collected qualitative survey data using a scenario-based questionnaire. Varieties of AI-assisted decision making processes were described and presented, and participants rated their attitude towards them. Participants also provided relevant demographic information; detailing proficiency with AI tools and medical and/or high-stakes career experience which allowed their responses to be categorised.\\

The second phase of the investigation involved a focus group discussion where participants discussed their sentiment towards AI, rated their attitude to varied scenarios involving high-stakes decision making and responded to statements made by the hosts relating to trustworthiness in high-stakes applications.\\

This section provides details of each phase of the investigation, including study design, the data collection process and the methods used for analysis.
% % Brief overview/ intro to methods (survey & focus group) before going into more detail
% % Brief discussion of why the methods were chosen

% \begin{itemize}
% \item To evaluate.../ address the aims of this work... We conducted a two-part investigation.
% \item First, we collected qualitative survey data using a scenario-based questionnaire. Different AI-assisted decision making processes were described and presented, and participants rated their attitude towards them.
% \item (Brief reason why we chose to survey) -  to allow us to create tabular datasets of trust vs transparency per demographic
% \end{itemize}
% \begin{itemize}
% \item The survey was designed to identify whether there were trends in the attitudes of different groups of participants towards the presented AI systems. 
% \item To gain insight into the possible reasoning behind participants attitudes, and to help identify any issues with the survey itself, we conducted a follow-up study comprising group interviews in the style of a focus-group.
% \item The design of this focus group was influenced by the results of the survey, so that we could specifically interrogate key findings. 
% \item Details of these methods are outlined in this section. 
% \item For full details on the ethical considerations and approval for this work, please see Appendix.
% \end{itemize}

% % Could also discuss why we chose to do survey + focus group, and not design workshops, interviews etc. JE - Do this in individual sections I think
% % DC



% \begin{itemize}
% \item The intention of the survey study
% \item Groups of interest, how and why they were chosen
% \item Defining key terminology and concepts
% \end{itemize}
% \begin{itemize}
% \item High-stakes decision making (what it could mean, what we are interested in, how we could investigate it, the relevance of healthcare professionals)
% \item Trustworthiness and Trust
% \item Transparency and Interpretability
% \end{itemize}
% \begin{itemize}
% \item The difficulty of discussing and conveying these concepts (not precisely defined, not well understood, ongoing area of research)
% \item We decided to focus on some key concepts:
% \end{itemize}
% \begin{enumerate}
% \item A black-box AI system...
% \item A glass-box AI system... an idealised interpretable system... The mechanisms of which are well understood, and the system provides some level of explanation or information about how it came to a decision, or which aspects of the data were most important in the output decision.
% \item Presence or absence of Human-AI interaction... presented in the simplest case: A human decision maker who is capable of analysing the data and coming to their own decision also has access to the above AI systems. Since it is not specified otherwise, it may be assumed that the human and AI system analyse the data and come to a decision independently, at which point the human makes an executive decision to agree with or override the output of the AI system.  
% \end{enumerate}
% \begin{itemize}
% \item By leaving the concepts open to interpretation, we allow the participants to specify their attitudes relative to their knowledge, understanding, preconceptions or biases about AI systems. 
% \end{itemize}
 
% \begin{itemize}
% \item Defining scenarios which capture the aspects of AI systems we are interested in describing.
% \end{itemize}
% \begin{itemize}
% \item Participants will include AI/ML experts as well individuals with no working knowledge of AI/ML. 
% \begin{itemize}
% \item We wanted to keep the scenarios relatable and understandable for all participants. 
% \item We chose a healthcare scenario that we considered to be well understood by the public.
% \item We chose to describe the AI systems such as to only convey the core concepts relevant to transparency and interpretability, without discussion of the practicalities and efficacy of the methods in real world scenarios. 
% \end{itemize}
% \end{itemize}




\subsection{Study 1 - Scenario-based Survey }
In this section we discuss the collection of survey data used as the basis for our analysis on the relationship between trust and transparency. Section \ref{study1_intentions} outlines the intended outcomes of this study. Section \ref{study1_design} provides definitions of trust and transparency for the reader and discusses potential biases in participant feedback, and the subsequent requirement to record demographic data alongside participants trust levels when considering AI tools.\\

We also hold a level of responsibility to ensure integrity in our findings and the protection of those involved in the study with regards their personal views, and any possible identifiable information. Section \ref{study1_recruitment_datacollection} details the efforts made to reduce and/or monitor potential biases in the survey data, while Section \ref{study1_ethics} outlines any risks to participants and the associated mitigating steps taken during this investigation.
\subsubsection{Intention of Study} 
\label{study1_intentions}
% It might make sense to separate some of these bullet points into a design section following the recruitment section.
The intention of this study was to gather quantitative data on the level of trust participants place in computer-aided decision making tools (herein referred to as "the AI tool/model") based on the level of transparency of the AI model. It was proposed that this data may enable us to draw inferences about the relationship between trust and transparency when considering AI tool transparency and user demographic. 

\subsubsection{Study Design} 
\label{study1_design}
The subjective nature of terms such as "trustworthiness" and "transparency" increases the risk of noisy data due to a participants' individual interpretations. This study aimed to mitigate this noise potential by clearly defining study scope and contextual boundaries for the participants.\\

We consider trustworthiness on a scale where the lowest measure is "not at all" and the highest measure is "completely". Table \ref{table:transparency_levels} defines four levels of model transparency which consider four conceivable scenarios. By gathering data from a group of participants on their level of trust in the decision-making process across these four levels of transparency we hope to establish, through investigation, whether any relationship exists. 

\begin{table}
\begin{center}
\begin{tabular}{ | c || p{11cm} |}
\hline
 Transparency & Scenario \\
\hline
\hline
 Level 1 & A human makes the decision with no AI model involved in the process. This is considered to be "fully transparent" as we can ask the human to justify any element of the decision-making process.  \\  
 \hline
 Level 2 & A human makes the decision with the aid of an AI tool which is interpretable. The human can query the tool and might be able to understand why the tool has made the decision. The human can override the decision of the AI tool. \\
 \hline
 Level 3 & A human makes the decision with the aid of an AI tool which is not interpretable. The human cannot query the tool and cannot understand why the tool has made the decision. The human can override the decision of the AI tool.  \\
 \hline
 Level 4 & An AI tool which is non-interpretable makes the decision. There is no human involvement in the decision-making process, and a human cannot override the decision of the AI tool. \\
\hline
\end{tabular}
\caption{Levels of transparency for four perceived scenarios.}
\label{table:transparency_levels}
\end{center}
\end{table}

Measuring trust for a population on a unified scale is a well-documented challenge [Sohns. A et al.] [Glaeser. E et al]. Evaluating an individual's level of trust is a multi-dimensional problem which depends on past experience, knowledge and interpretation of terminology regarding levels of trust. Table \ref{table:study1_constraints} defines constraints for the data-capturing process deemed necessary to correctly capture variations in trust for different levels of model transparency. In the following paragraphs, we justify these constraints in turn.\\

% Covid Diagnosis
 Firstly, we assume that an individual's level of trust in an AI tool will vary considerably based on the potential impact of that decision. For example, the general public might be passive when AI tools are utilised to suggest nearby restaurants during the process of ordering food, but could feel some level of concern if a judicial organisation announced that it would apply an AI tool to determine all criminal sentencing procedures for future convictions. We attempt to standardise the data collection process by fixing the stake of the decision in question, establishing \textbf{Constraint 1}. \\
 
Despite fixing the stake of the decision, it is perceived that individuals who routinely make high-stakes decisions may respond differently to those who do not. We asked study participants to self-rate on an ordinal scale of one to ten whether they felt they routinely made high-stakes decisions. A simple and broad definition was given alongside some examples of relevant professional roles.\begin{quote}
    "By 'high-stakes decision making' we mean making decisions that are highly impactful. High-stakes decisions occur in many settings, such as legal judgements, healthcare, insurance pricing, offering mortgages, stock trading, etc.".
\end{quote}

This method allows participants from all backgrounds and professions to identify themselves as individuals involved in high-stakes decision making only if they believe it is appropriate to do so. Similarly to high-stakes decision makers, someone who works in a hospital may consider the decision-making process for a medical diagnosis very differently to someone who has never worked in a medical setting. This leads to \textbf{Constraint 2} and \textbf{Constraint 3}. \\

%  Level of experience in AI
Finally, we must consider whether any participants have experience within the field of AI; an AI specialist may exhibit bias towards or against AI tools based on their personal experience, resulting in \textbf{Constraint 4}. \\

By applying these constraints to our data collection process, we aim to understand whether trust in AI tools is dependent on factors other than model transparency. This is primarily to prevent any false conclusions from the data but also enables a deeper analysis into human trustworthiness of AI based on application-specific experience and/or knowledge of AI.

\begin{table}
\begin{center}
\begin{tabular}{ | c | p{11cm} |}
\hline
 Constraint 1 & Participants are asked to consider a situation where an AI tool is used to aid the decision of whether or not they have contracted COVID-19. \\  
 \hline
 Constraint 2 & Any participant must inform the study of their experience of high-stakes decision making. \\
 \hline
Constraint 3 & Any participant must inform the study whether they work, or have worked, in a healthcare setting. \\
 \hline
 Constraint 4 & Any participant must inform the study whether they have worked with, researched or developed AI tools. \\
\hline
\end{tabular}
\caption{Study 1 Constraints}
\label{table:study1_constraints}
\end{center}
\end{table}

\subsubsection{Recruitment and Data Collection}
\label{study1_recruitment_datacollection}

Section \ref{table:study1_constraints} identifies potential sources of bias in the data due to experience with AI, in healthcare, or due to environments wherein high-stakes decisions are frequent or uncommon. We therefore not only asked participants to provide information about any potential bias, but also aimed to recruit from varied sources with the objective of equal representation across all axis. We justify each recruitment stream as follows:

\begin{itemize}
    \item Quasi-random sampling of the public through Twitter, LinkedIn, and personal networks; While not truly random, as all study organisers (and consequently our networks) are somewhat linked by experience, we believed we could reach participants with largely low experience working with AI and/or in healthcare, and varied experience with high-stakes decisions. (We received approximately 100 responses from one study organisers' secondary anonymised twitter account which is believed to be a truly random collection of people from around the world).
    \item Other AI-focused Centres for Doctoral Training (CDTs); By targeting AI researchers in other CDT's we hoped to collect responses from a demographic of proportionately high experience with AI tools.
    \item Healthcare-related mailing lists; This recruitment strategy aimed to provide a significant number of participants with experience in a healthcare setting, where high-stakes decisions can be common.
    \item Reddit communities; Additional participants were recruited from reddit communities [r/london, r/machinelearning, r/coronavirusrisk] to bolster specific demographic participation.
\end{itemize}

Data is gathered in the form of a survey. We ask participants;

\begin{enumerate}
    \item Whether they have worked in a healthcare setting
    \item How often they are involved in making high-stakes decisions
    \item To rank their level of experience with AI
\end{enumerate}

These questions pertain to the constraints defined in Table \ref{table:study1_constraints}. We then proceed to describe four scenarios to the participant, where transparency in the decision-making process is aligned with each consecutive level defined in \ref{study1_design}. For each scenario we ask users to rank their level of trust on a scale from 1-10 where 1 corresponds with "Not at All", and 10 corresponds with "Completely". An example of this question format is presented in Figure \ref{fig:example_survey_question}.

\begin{figure}
    \fbox{\includegraphics[width=\linewidth]{graphics/example_survey_question.png}}
    \caption{Study 1 - Survey Example Question}
  \label{fig:example_survey_question}
\end{figure}

\subsubsection{Ethical Considerations} 
\label{study1_ethics}

When surveying a population, several key ethical considerations must be addressed. Table \ref{table:study1_ethical_considerations} lists these considerations and the actions taken by the research team to address each consideration.

\begin{table}
\begin{center}
\begin{tabular}{ | p{3cm} | p{8cm} |}
\hline
 Ethical Consideration & Action \\
 \hline
 \hline
 Appropriate inclusion and exclusion criteria & The research team excluded minors and anyone who may be mentally or emotionally vulnerable when recruiting from personal networks. Due to the anonymity of participant submissions, recruitment from online platforms (Twitter, LinkedIn) could not be monitored and we therefore performed recruitment with the assumption that these organisations have acceptable use policies and appropriate restrictions on age within their terms of service.\\
 \hline
Informed Consent & An information sheet ###appendix? was provided to each participant, detailing the research and any potential risks. Participants were presented with a choice to proceed accompanied with the following message:

\begin{quote}
    By clicking "Next" you confirm that you've read the full information about this study and consent to participate.
\end{quote} \\
 \hline
 The safety and well-being of any participants & In accordance with the University of Bristol ethics approval process, Table \ref{table:study1_risks} identifies physical, psychological, legal and social risks associated with our research and any actions taken to mitigate these risks. \\
\hline
The protection of any participants data & All data was anonymised, with no identifiers included in the questionnaire. A separate email database was recorded for compensation of those who completed the questionnaire. \\
\hline
Compensation & The survey took an average of approximately seven minutes to complete. To compensate participants for their time, each participant was given the opportunity to enter their email address into a separate database (ensuring anonymity) to be entered into a prize draw for one of two £25 amazon vouchers. \\
 \hline
\end{tabular}
\caption{Study 1: Ethical Considerations}
\label{table:study1_ethical_considerations}
\end{center}
\end{table}


\begin{table}
\begin{center}
\begin{tabular}{ | p{6cm} | c | p{6cm} |}
\hline
 Risk & Likelihood & Mitigation \\
 \hline
 \hline
 Physical: None & N/A & N/A \\
 \hline
 Psychological: People might start to worry about their job safety or the quality of a diagnosis based on computer-aided decision-making tools should they require actual diagnosis in future. People might feel bad about themselves not knowing much about computer-aided decision-making tools. & Low & The survey is clearly hypothetical; it does not mention a concrete diagnosis and we clearly ask people to imagine being a patient. The survey does not explore or mention computer-aided decision-making tools replacing people's jobs. The survey will be designed to use simple ways to talk about computer-aided decision-making tools to ensure a wide range of people can understand the questions. \\
 \hline
 Legal: : None & N/A & N/A \\
 \hline
Social: Risk of being identified through combining multiple quasi-identifiers. & Low & Survey data is submitted anonymously. Careful consideration of collected potential quasi-identifiers to ensure minimal chance of identification through combining multiple quasi-identifiers. Exclusion of collecting any non-essential data such as sex, place of work etc.\\
\hline
\end{tabular}
\caption{Study 1: Identifiable Risks to Participants}
\label{table:study1_risks}
\end{center}
\end{table}





% add details on how we requited participants, how many responded and where from.

\subsubsection{Data Analysis}
% ALEX TO PULL STUFF IN?

\subsection{Study 2 - Focus Group}

This section details the focus group which was carried out subsequent to collection of survey data in Study 1. Section ### outlines the intended outcomes of his study. Section ### details pros and cons?.

Section ### details the data gathering process and Section ### outlines any ethical considerations which were to be addressed prior to hosting the focus group session.


A similar questionnaire to the survey was used to gather data from the focus group participants. Participants were given the opportunity to express any reactions to the results of the questionnaire, and the focus group also served as a platform for discussion of our findings from the survey results. allowed us to query the participant any results from the questionnaire and to clarify their  investigated our findings.

 



\subsubsection{Notes on results for discussions:}








\subsubsection{Brainstorm}
\begin{itemize}
    \item Correlation analysis
    \item Clustering analysis
\end{itemize}




\subsubsection{Bit of maths}

In order for the AI to have any impact on the doctor, the doctor has to actually take the AI's diagnosis into account. For an AI diagnosis $A^x, x\in \{+,- \}$ and an original doctor diagnosis $D^x, x\in \{+,- \}$. As shown visually in Fig. \ref{fig:coop_diagram}, we can model a bias factor $f^x_{\textrm{Agree/Disagree}}$. The doctor can be assumed to carry a unique bias towards agreeing or disagreeing with the AI, dependent on the AI's diagnosis, which here we've modelled as a factor change to the doctor's original efficacy.

\tikzstyle{arrow} = [thick,->,>=stealth]
\tikzstyle{node} = [rectangle, text centered, draw = black, fill = white!255, minimum width = 2.5cm]
\tikzstyle{correct} = [rectangle, text centered, draw = black, fill = green!50, minimum width = 2.5cm]
\tikzstyle{incorrect} = [rectangle, text centered, draw = black, fill = red!30, minimum width = 2.5cm]

\begin{figure}[h]
\centering

\begin{tikzpicture}[node distance=2cm]
\node (patient) [node] {Patient symptoms};

\node (AI_pos)  [node, right of = patient, xshift = 2cm] {AI positive diagnosis};
\node (AI_neg)  [node, below of = patient, yshift = -0cm] {AI negative diagnosis};

\node (Pos_pos) [correct, right of = AI_pos, xshift = 4cm] {Doctor agrees, positive diagnosis};
\node (Pos_neg) [incorrect, above of = AI_pos, yshift = 0cm] {Doctor disagrees, negative diagnosis};

\node (Neg_pos) [incorrect, right of = AI_neg, xshift = 5cm] {Doctor agrees, negative diagnosis};
\node (Neg_neg) [correct, below of = AI_neg, yshift = -0cm] {Doctor disagrees, positive diagnosis};

\draw [arrow] (patient) -- node[anchor=south] {$A^+$} (AI_pos);
\draw [arrow] (patient) -- node[anchor=west] {$A^-$} (AI_neg);

\draw [arrow] (AI_pos) -- node[anchor=south] {$f^+_{\textrm{Agree}}D^+$} (Pos_pos);
\draw [arrow] (AI_pos) -- node[anchor=west] {$f^+_{\textrm{Disagree}}D^-$} (Pos_neg);

\draw [arrow] (AI_neg) -- node[anchor=south] {$f^-_{\textrm{Disagree}}D^+$} (Neg_pos);
\draw [arrow] (AI_neg) -- node[anchor=west] {$f^-_{\textrm{Agree}}D^-$} (Neg_neg);

\end{tikzpicture}

\caption{The process of AI-cooperative diagnosis}
\label{fig:coop_diagram}
\end{figure}

In this model the probability of a correct diagnosis for a patient with COVID is:

\begin{equation}
    P(\textrm{correct}) = (f^+_{\textrm{A}}A^+ + f^-_{\textrm{D}}A^-) D^+
\label{eqn:correct_coop}
\end{equation}

with $f^+_{\textrm{A}}$ shorthand for $f^+_{\textrm{Agree}}$, and the same shortening notation for $f^-_{\textrm{Disagree}}$. We can assume that the probability of a correct diagnosis is limited to $0 \leq P(\textrm{correct}) \leq 1$, which sets limits

\begin{equation}
    0 \leq f^+_{\textrm{A}}A^+, f^+_{\textrm{D}}A^- \leq 1/D^+
\end{equation}


In this model the patients diagnosis is only improved if $(f^+_{\textrm{A}}A^+ + f^-_{\textrm{D}}A^-) > 1$, otherwise the chance of a COVID positive patient being correctly diagnosed decreases. Important to note is that if $f^+_A \textrm{ and } f^-_D = 1$, ie if the doctor has no interpretation of the AI's diagnosis, then Equation \ref{eqn:correct_coop} becomes

\begin{equation}
    P(\textrm{correct}) = (A^+ + A^-) D^+ = D^+
\end{equation}

the effect of which being that the AI has no impact on the doctor's efficacy. 


% \begin{figure}[h]
% \centering

% \begin{tikzpicture}

% \draw[opacity=0.5,fill=green!50] (0pt, 6cm) -- (0pt, 9cm) -- (9cm, 0pt) -- (6cm, 0pt) -- (0pt, 6cm);
% \draw[opacity=0.5,fill=red!30] (0pt, 0pt) -- (0pt, 6cm) -- (6cm, 0pt) -- (0pt, 0pt);

% \draw[thick,->] (0,0) -- (10,0) node[anchor=north west] {$f^+_{\textrm{A}}A^+$};
% \draw[thick,->] (0,0) -- (0,10) node[anchor=south east] {$f^-_{\textrm{D}}A^-$};

% \draw (9cm,2pt) -- (9cm,-2pt) node[anchor=north] {$1/D^+$};
% \draw (2pt, 9cm) -- (-2pt, 9cm) node[anchor=east] {$1/D^+$};
% \draw (0pt, 9cm) -- (4.5cm, 4.5cm) node[anchor=west] {Upper probability limits};
% \draw (4.5cm, 4.5cm) -- (9cm, 0pt) node[anchor=west] {};

% \draw (6cm,2pt) -- (6cm,-2pt) node[anchor=north] {$1; f^+_A = 1/A^+$};
% \draw (2pt, 6cm) -- (-2pt, 6cm) node[anchor=east] {$1; f^-_D = 1/A^-$};
% \draw (0pt, 6cm) -- (3cm, 3cm) node[anchor=west] {Improved diagnosis};
% \draw (3cm, 3cm) -- (6cm, 0pt) node[anchor=west] {};

% \draw [dotted] (4.5cm, 1pt) -- (4.5cm, 1.5cm); 
% \draw [dotted] (4.5cm, 1.5cm) -- (1pt, 1.5cm); 
% \draw (4.5cm,0pt) -- (4.5cm,-2pt) node[anchor=north] {$A^+$};
% \draw (0pt,1.5cm) -- (-2pt, 1.5cm) node[anchor=east] {$A^-$};

% \draw [dashed] (3cm, 1pt) -- (3cm, 3cm); 
% \draw [dashed] (3cm, 3cm) -- (1pt, 3cm); 
% \draw (3cm,0pt) -- (3cm,-2pt) node[anchor=north] {$D^+$};
% \draw (0pt,3cm) -- (-2pt, 3cm) node[anchor=east] {$D^-$};

% \draw (1pt,1pt) -- (-1pt, -1pt) node[anchor=north east] {$0,0$};

% \end{tikzpicture}

% \caption{\centering Regions of diagnosis efficacy in a human-AI cooperative scenario, with $f^{+/-}_{A/D}$ bias factors when interpreting the AI diagnosis $A^{+/-}$, and the non-AI doctor diagnosis $D^{+/-}$. Not to scale with the values in the survey scenarios.}
% \label{fig:my_label}
% \end{figure}

% \begin{figure}[h]
% \centering

% \begin{tikzpicture}

% \draw[opacity=0.5,fill=red!50] (0cm, 4cm) -- (4cm, 4cm) -- (4cm, 0cm) -- (0cm, 0cm) -- (0cm, 4cm);

% \draw[opacity=0.5,fill=blue!30] (0cm, 4cm) -- (0cm, 6cm) -- (4cm, 6cm) -- (4cm, 4cm) -- (0cm, 4cm);

% \draw[opacity=0.5,fill=green!30] (4cm, 4cm) -- (4cm, 6cm) -- (6cm, 6cm) -- (6cm, 4cm) -- (4cm, 4cm);

% \draw[pattern=north west lines, pattern color=blue!30, opacity=0.5] (4cm, 0cm) -- (4cm, 4cm) -- (6cm, 4cm) -- (6cm, 0cm) -- (4cm, 0cm);

% \draw (2cm,5cm) -- (2cm,5cm) node[anchor=north] {A};
% \draw (5cm,5cm) -- (5cm,5cm) node[anchor=north] {B};
% \draw (2cm,2cm) -- (2cm,2cm) node[anchor=north] {C};
% \draw (5cm,2cm) -- (5cm,2cm) node[anchor=north] {D};

% \draw (3cm,0pt) -- (3cm,0pt) node[anchor=north] {Glass box trust factor change};
% \draw (0pt,3cm) -- (0pt,3cm) node[anchor=east, rotate=-90, xshift=2cm, yshift=-7pt]  {Black box trust factor change};

% \end{tikzpicture}

% \caption{\centering Interpretation schema for cooperative improvement plots. \textbf{A}: Black box improves diagnosis, glass box makes diagnosis worse \textbf{B}: Both glass box and black box improve diagnosis \textbf{C}: Neither glass box or black box improve diagnosis \textbf{D}: Glass box improves diagnosis, black box makes diagnosis worse}
% \label{fig:quadrant explanation}
% \end{figure}



From this model we can define some doctor profiles based on how they might interact with an AI tool:

\begin{itemize}
    \item \textbf{$f^+_A, f^-_D = 1/D^+$:}  A doctor who trusts the AI's positive diagnoses completely, but also catches every single mis-classification by the AI
    \item \textbf{$f^+_A, f^-_D = 0$:}      A doctor who never agrees with a positive AI diagnosis, and always agrees with a negative AI diagnosis
    \item ETC, FLESH OUT LATER
\end{itemize}

\subsection{Focus Group}
\begin{itemize}
    \item Goals, how does it follow from survey data? (does trust mean you will be happy to use the AI?)
    \item Who do we want to talk to? How will we use the information?
    \item 
\end{itemize}

A focus group study was devised as a follow-up to the survey study, to gain insight into the reasoning and motivation behind survey responses, and to uncover topics of interest and concern as well as differences in opinion surrounding AI decision support tools.  



\subsubsection{Recruitment}

Recruitment was performed via the survey from Section [link to section?] - participants were presented with an option to provide an email address if they were interested in being contacted to participate in the focus group. Following the collection and analysis of survey data, a recruitment survey was sent to the participants of the first survey who agreed to be contacted. The recruitment survey was designed to gather a small amount of demographic information so that an appropriate spread of participants were included [change wording].

[What is the requested demographic information? Use questionnaire data or ask for more? Inclusion of BAME and gender?]

Subreddits:
\begin{itemize}
    \item r/machinelearning
    \item r/london - awaiting approval
    \item r/coronavirusuk - awaiting approval
\end{itemize}

This demographic information was used to recruit at least one person from the following categories:

\textbf{Category 1: Experienced Senior AI Expert}
\begin{itemize}
\item High self-reported rating in AI expertise.
\item Not a medical professional.
\item Mid to high self-reported involvement in high-stakes decision making
\end{itemize}
(Ideally in a senior role or management position).

\textbf{Category 2: AI Specialist}
\begin{itemize}
\item Mid to high self-reported rating in AI expertise.
\item Not a medical professional.
\item Low self-reported involvement/responsibility in high-stakes decision making.
\end{itemize}

Ideally not in a senior role or management position, could be early career or still in education)

\textbf{Category 3: Experienced Senior Healthcare Expert}
\begin{itemize}
\item Low to Mid self-reported rating in AI expertise
\item Is a medical professional, 
\item High self-reported involvement/responsibility in high-stakes decision making
\end{itemize}

Ideally in a senior role or management position in a clinical hospital setting.

\textbf{Category 4: Healthcare Specialist}
\begin{itemize}
\item Low self-reported rating in AI expertise
\item Is a medical professional.
\item Mid to High self-reported involvement/responsibility in high-stakes decision making
\end{itemize}

Ideally in a role with some responsibility for high-stakes decision making, or a role in which they provide information and evidence which supports others in making high stakes decisions. 

This individual should be employed in a clinical hospital setting (could still be in education and training, but should currently be undertaking practical rotations, or have practical responsibilities in a clinical hospital setting). 


\textbf{Category 5: Wildcard}
Any combination of self-reported traits.
Could be a member of the public.
Could be a second member of any previous category. 
Possibility of being a clinical informatician, or other clinical computer scientist.
Possibility of being a software engineer who has experience with developing medical devices. 

[Explain why you wanted to recruit people at the end of each spectrum, and avoid people who cross the demographics]

\subsubsection{Topics of Discussion}



\subsubsection{Data Gathering and Analysis}
Interviews were conducted via Microsoft Teams [University of Bristol server?]. The audio from the entire session was recorded, and stored temporarily on a secure server. The session audio was used create an anonymised transcript, then promptly deleted. 

Thematic analysis was performed on the focus group transcript using NVivo [cite]. 

[Describe how you will do thematic analysis on the focus group data].








\newpage
\section{Findings - Survey}
%Describe our findings from user research
%Evaluation of our findings
%\subsection{Survey}

\subsection{Dis-aggregation and grouping}

The demographic questions at the start of the survey allow us to investigate how different groups attribute trust in the scenario-based questions. We split into three bins along the AI expertise and high-stakes questions. 

These bins are:

\begin{itemize}
    \item Low: $(f_{AI}, f_{stakes}) \leq 3$
    \item Medium: $3 < (f_{AI}, f_{stakes}) \leq 7$
    \item High: $7 < (f_{AI}, f_{stakes})$
\end{itemize}
% $1 \leq (f_{AI}, f_{stakes}) \leq 3$, $3 < (f_{AI}, f_{stakes}) \leq 7$, and $7 < (f_{AI}, f_{stakes}) \leq 10$.
We can also use the profiling questions from the survey to dis-aggregate the survey participants into distinct cohorts that are particularly relevant in discussions of AI transparency: 

\begin{itemize}
    \item Medical professional: Answered "yes" to relevant question
    \item AI Expert: Not a medical professional, and AI experience level reported as $\geq 7$
    \item Public (model patient): Neither a medical professional nor an AI expert
\end{itemize}

These are intended to represent the three agents in each cooperative scenario - the doctor being the medical professional, the patient being the public, and the AI expert standing in for the AI.

\subsection{Mean Results}

To understand the survey results at their simplest we can look at their group averages. Figure \ref{fig:mean_survey_results} shows the mean trust levels for each scenario for our 3 demographic characteristics: AI expertise, medical and high stakes decision makers. We see that on average the Doctor and glass box has the highest trust level, the doctor on their own has second, doctor and black box third and black box on its own fourth. 

When grouped by AI expertise, higher levels of expertise lead to more trust in the cooperative scenarios. No expertise leads to more trust with the doctor on their own and less trust with either black box scenario. Interestingly, the high AI expertise group trust the black box on its own the least. 

Grouping into medical professionals produces no significant trust differences across the four scenarios compared to the rest of the participant population. However, Figure \ref{subfig:high stakes} shows that the greater the high stakes decision making, the more trust is placed in the black box on its own scenario.

\begin{figure}[ht]
  \begin{subfigure}[t]{.45\textwidth}
    \centering
    \includegraphics[width=\linewidth]{graphics/survey_averages/survey_average.png}
    \caption{All demographics.}
  \end{subfigure}
  \begin{subfigure}[t]{.45\textwidth}
    \centering
    \includegraphics[width=\linewidth]{graphics/survey_averages/survey_AI_grouped.png}
    \caption{Grouped by AI expertise.}
  \end{subfigure}
  \medskip
  \begin{subfigure}[t]{.45\textwidth}
    \centering
    \includegraphics[width=\linewidth]{graphics/survey_averages/survey_medics_grouped.png}
    \caption{Grouped by medical professional.}
  \end{subfigure}
  \begin{subfigure}[t]{.45\textwidth}
    \centering
    \includegraphics[width=\linewidth]{graphics/survey_averages/survey_stakes_grouped.png}
    \caption{Grouped by high stakes decision making.}
    \label{subfig:high stakes}
  \end{subfigure}
  \caption{Mean results of demographics from the survey results for each 4 scenarios. AI expertise and high stakes decision making numerical values are binned at: Low (0, 4], Medium (4, 7], High (7, 10]. All of the plots use the following colour scheme. Blue: Scenario 1 (doctor on their own), Orange: Scenario 2 (doctor with glass box), Green: Scenario 3 (doctor with black box), Red: Scenario 4 (black box on its own).}
  \label{fig:mean_survey_results}
\end{figure}

\subsection{Scenario Ranking}


Using each participant's trust level in each scenario we are able, for each participant, to rank their scenario preference. We also group using the demographic bins detailed above. The ranking was calculated using panda's rank function, with ties in ranking resolved using panda's 'average' method\cite{mckinney-proc-scipy-2010}.

In total there were 9 participants who have given each scenario the same number of trust, 49 who have given each scenario a different number of trust, 105 who have given two scenarios the same trust and 115 who have given three scenario the same trust.  To be able to compare the charts across groupings, regardless of how many people are in which group, the proportion of answers is normalised to a percentage of that group with that ranking: $100*(N_{\textrm{Rank}}/N_{\textrm{In Group}})$\%

%number of answers/(total number of answers)*100.

% For AI experience and frequency of high-stakes decision making the rating from 1-10 was binned into three bins: Low, Medium, High using panda's cut function\cite{mckinney-proc-scipy-2010}. The resulting bins are both were low $(0.991, 4.0] <$ medium $(4.0, 7.0] <$ high (7.0, 10.0].

From Fig. \ref{fig:overall-ranking} we can see that the black box (BB) was most frequently ranked the lowest whereas the glass box together with the doctor (DR+GB) was most frequently ranked highest.

While the actual percentage of ranking has to be taken with a pinch of salt given that only 49 participants have given each scenario a different trust level and with it a different rank. The average method used to resolve such ties impacts the number of participants e.g ranking the BB lowest but it does not impact that no matter what the tie resolution strategy is the BB gets the lowest rank.

The black box by itself is, regardless of tied-rank resolution strategy, the lowest ranked scenario.

It is interesting to look at each of the ranks 1-4 and the order in which the different scenarios are appearing in that rank:

For the lowest rank - a ranking of 1 - the BB was by far the lowest ranking scenario followed by the Dr on their own, then the black box with the doctor. The glass box with the doctor was  least frequently ranked the lowest. Figure? \ref{fig:overall-ranking}

For the highest rank - ranking of 4 - the glass box with the dr was ranked most often the highest, the Dr on their own second most often, the black box with the Dr third most often and the black box on it's own was ranked least often the highest.\ref{fig:overall-ranking}

\begin{figure}[H]
    \centering{\includegraphics[width=0.8\textwidth]{graphics/ranking/rankingOverallNormalisedAvarageTies.png}}
    \caption{Overall ranking for each of the four scenarios. Rank 1 is the lowest, rank 4 the highest ranking.}
    \label{fig:overall-ranking}
\end{figure}

Looking at how the ranking changes based on if the participant works in healthcare or not we can see that the order for the highest rank (4) and lowest rank (1) stay the same as for the overall chart. However we can also see that people who work in healthcare more frequently rate the Dr on their own as the second lowest scenario that gets a rank Figure? \ref{fig:ranking-healthcare}..

\begin{figure}[H]
    \centering{\includegraphics[width=0.8\textwidth]{graphics/ranking/ranking-healthcare-vs-notNormalised-average-ties.png}}
    \caption{Ranking based on participants working in healthcare or not for each of the four scenarios. Rank 1 is the lowest, rank 4 the highest ranking.}
    \label{fig:ranking-healthcare}
\end{figure}

If we look at how the ranking changes with regards to how much AI experience the participants have we again see no change compared to the overall ranking for the highest and lowest rank. We can however see that black box together with the doctor is more frequently ranked in rank 3 for participants with medium and high AI experience and that the black box on it's own is more frequently getting a rank 3 then the Dr on it's own does\ref{fig:ranking-ai}.

\begin{figure}[H]
    \centering{\includegraphics[width=0.8\textwidth]{graphics/ranking/ranking-ai-experience-normalised-average-ties.png}}
    \caption{Ranking based on participant's AI experience of low, medium or high experience for each of the four scenarios. Rank 1 is the lowest, rank 4 the highest ranking.}
    \label{fig:ranking-ai}
\end{figure}

Finally if we look at how the frequency how often a participants makes high-stakes decision changes which scenario gets ranked lowest or highest we can see a change in the high high-stakes decision maker compared to the overall ranking. High high-stakes decision maker rank the Dr on their own as the lowest trust scenario and the black box on its own as the second lowest lowest trust scenario\ref{fig:ranking-highstakes-decision}..

\begin{figure}[H]
    \centering{\includegraphics[width=0.8\textwidth]{graphics/ranking/ranking-high-stakes-decisions-normalised-average-ties.png}}
    \caption{Ranking based on participant's frequency of high-stakes decision making of low, medium or high frequency for each of the four scenarios. Rank 1 is the lowest, rank 4 the highest ranking.}
    \label{fig:ranking-highstakes-decision}
\end{figure}

% \subsection{Data Analysis Survey}

% /JE

% Highlight the use of covid diagnosis makes people apply the risk to themselves. So we get more thoughtful/emotive responses.

% Split into Study one and Study two.
% How did we pick the 5 people from the focus group





%AD





\subsection{Demographic Motivated Impact of AI}


\subsubsection{Additional Features}

We use the results of the first scenario, the trust in the doctor with no AI component $T_D$, as a baseline to assess the impact of AI-cooperation on diagnosis trust levels. We denote the trust level reported in these cooperative scenarios based on the nature of their model. Scenario two, with the interpretable or "glass box'' model is $T_{GB}$, and scenario three with the "black box'' model $T_{BB}$.

We normalise the survey scale of [1,10] to [0,1] for $T_D, T_{BB}, T_{GB}$, via $T_i \rightarrow (T_i - 1) * (10/9)$, then produce two new features based on the percentage factor change in trust:

\begin{equation} \label{eqn:change_vs_doctor}
    f_i = 100 \cdot (T_i / T_D)
\end{equation}

These features allow analysis of whether participants view the cooperative use of AI as beneficial ($f_i > 0$), neutral ($f_i = 0$), or inhibitory ($f_i < 0$) to the diagnosis. 

% We plot the results in Figure \ref{fig:improvement}, along with an interpretation schema, with participants dis-aggregated into three cohorts given the following criteria:



% The survey was closed after XXX days, with XXX participants. We use the two key profile features - AI experience level and whether the participant is a medical professional - to dis-aggregate into three demographic bins:


We then apply Equation \ref{eqn:change_vs_doctor} and analyse the changes in participant trust in the two doctor/AI cooperative scenarios using Figure \ref{fig:improvement}.

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.85\textwidth]{graphics/Doctor_improvements_cooperation_scatter.jpg}
    \caption{\centering Perceived improvements in trust in scenarios 3 and 4. Percentages are calculated using $\leq 0$ and $>0$ basis.\hspace{\textwidth} Interpretation schema: \textbf{A}: Black box improves diagnosis, glass box makes diagnosis worse \hspace{\textwidth} \textbf{B}: Both glass box and black box improve diagnosis. \textbf{$B_1$} black box is preferred, \textbf{$B_2$} glass box is preferred, $B_3$ neither is preferred. \textbf{C}: Neither glass box nor black box improve diagnosis \textbf{D}: Glass box improves diagnosis, black box makes diagnosis worse}
    \label{fig:improvement}
\end{figure}

Through Figure \ref{fig:improvement} we derive how different cohorts view the use of AI in cooperative scenarios. In all cohorts, a significant proportion of participants view the use of AI as beneficial with both models - 46\% of the public, 53\% of medical professionals, and 60\% of AI experts. This can be assumed to match the progression of technical expertise between cohorts.

Only a minority of each cohort view neither AI-cooperative scenario as beneficial. In all cohorts the glass box model is preferred over the black box model, but only by a small margin, with ratios $f_{BB}:f_{GB}$ of 1:1.17, 1:1.125 and 1:1.40 for the public, medical professionals and AI experts respectively. Table \ref{tab:demographic_trust_change} shows the percentages from Figure \ref{fig:improvement}.

\begin{table}[h]
\begin{tabular}{c||c|c|c}
     & Public & Medics & AI Experts \\
     \hline \hline
    GB and BB inhibit (C)            & 29\% & 23\% & 16\%\\ \hline
    GB inhibits, BB improves (A)     & 5\%  & 2\%  & 5\% \\
    GB improves, BB inhibits (D)     & 21\% & 21\% & 19\%\\  \hline
    GB and BB improve (B)            & 46\% & 53\% & 60\%\\ \hline
    GB and BB improve (equal)        & 17\% & 19\% & 31\%\\
    GB and BB improve (GB better)    & 10\% & 15\% & 17\%\\
    GB and BB improve (BB better)    & 19\% & 19\% & 12\%\\ 
\end{tabular}
\caption{\label{tab:demographic_trust_change}Perceived changes in trust of a diagnosis from an AI/doctor cooperation, dis-aggregated into previously described demographics. Note: Relevant percentages may not sum to 100\% due to rounding errors.}
\end{table}
%\subsection{Correlation Analysis}

Using Table \ref{tab:demographic_trust_change} we can see a more subtle dynamic in participant trust ratings by inspecting the proportions of each cohort that view both cooperative AI scenarios as improving the diagnosis. The proportion that view both glass box and black box as useful increases with presumed technical level (public, medics then AI experts). However AI experts who view both scenarios as beneficial to the diagnosis are less likely to favour the black box compared to the glass box. Adding to this, as presumed AI technical expertise increases (again public, medics, then AI experts), the proportion explicitly favouring the glass box scenario also increases.

This analysis has shown that across all cohorts at least some version of doctor-AI cooperation is seen as beneficial to a diagnosis, with only a minority of each cohort viewing both presented scenarios as inhibitory to assigned trust. These proportions, 29\%, 23\% and 16\% for the public, medics and AI experts respectively, decrease with presumed level of technical expertise. Similarly, the proportion of participants in each cohort that view both cooperative scenarios as beneficial to a scenario follow presumed technical expertise, with 46\%, 53\% and 60\% for the public, medics and AI experts respectively.

\subsection{Trust Motivated Clustering}

Having disaggregated into profiling features, then analysed the trust levels of these cohorts, we can instead do the opposite. By starting with trust levels and finding distinct groups of how they are distributed, we can construct cohorts based on trust levels, then use profiling features to find the characteristics of these groups.

Insights can be gained into the results of the survey by applying unsupervised methods. These do not rely on any labels or targets, as would be the case in supervised or semi-supervised methods, so avoid many of the biases that such methods would carry. In particular we apply Uniform Manifold Approximation and Projection (UMAP) \cite{McInnes2018} as dimensionality reduction, then Hierachical Density-Based Spatial Clustering And Noise (HDBSCAN) \cite{McInnes2017} as unsupervised clustering on the resulting UMAP embedding. Both UMAP and HDBSCAN use similar methods, and make similar assumptions, so work well in tandem as an unsupervised classification method.

UMAP makes the assumption that data is distributed on a uniform manifold, but that the data-space is warped. Using this assumption local distance metrics are learnt, then used to construct a distance graph of the data. A lower-dimensional embedding is then formed, which aims to conserve the form of the high-dimensional space, using a stochastic graph layout. HDBSCAN, similarly, constructs a minimum spanning tree based on distances in the data-space, then applies various sub-algorithms to find clusters. Full explanation of these algorithms is beyond the scope of this work, so the reader should consult the relevant papers for more information. 

We apply UMAP, then HDBSCAN to the resulting embedding, and present the results in Figures \ref{fig:embedding}, \ref{fig:cluster_means}. UMAP produces four distinct clusters, which are recognised by HDBSCAN. HDBSCAN is designed to find clusters in noisy data, and so can identify points as noise, but none are found here, indicating that clusters are clearly defined.

\begin{figure}
\centering
\begin{subfigure}{0.5\textwidth}
  \centering
  \includegraphics[width=\linewidth]{graphics/UMAP_HDBSCAN.jpg}
  \caption{}
%   \caption{The UMAP embedding produces four (visually) distinct clusters, which are also identified by HDBSCAN.}
  \label{fig:embedding}
\end{subfigure}%
\begin{subfigure}{0.55\textwidth}
  \centering
  \includegraphics[width=\linewidth]{graphics/clustering_means.jpg}
  \caption{Cluster means across all four scenarios, along with profiling features.}
  \label{fig:cluster_means}
\end{subfigure}
\caption{The results of unsupervised clustering with UMAP and HDBSCAN. In Figure \ref{fig:cluster_means} profile features (AI, Medical, Stakes) are normalised by the whole population. Scenario trust levels are moved to a scale [0-1] for greater clarity in the heatmapping.}
\label{fig:test}
\end{figure}

The clusters identified by this process are distinct in how the participants within them rated trust in the different scenarios. Clusters 1 and 3 follow the same trend, with the highest trust in the glass-box cooperative scenario, and lowest in the solo black-box. However, cluster 3 has significantly lower trusts across all scenarios. Cluster 3 is significantly larger than the other clusters, representing 64\% of the participant pool.

Clusters 0 and 2 have the lowest level of trust placed in the doctor, with the solo black-box scenario scoring higher, in contrast to the opposite relationship in clusters 1 and 3. In both cluster 0 and 2 the highest trust is in the black-box cooperative scenario. Cluster 2 is the only cluster in which the glass-box cooperative scores lower than the solo doctor.

This analysis of only prescribed trusts leads to an intuitive split: In evaluating the compromise between transparency and efficacy, those in clusters 0 and 2 prescribe more weight to efficacy, and those in clusters 1 and 3 weight transparency over efficacy. Those in cluster 0 are perhaps closer to even balancing of transparency and efficacy than those in cluster 2, as shown by their higher trust levels in the glass-box scenario.

Cluster 1 probably represents a population of AI researchers - high average AI knowledge, but low stakes decision making and a small medical proportion. These participants rate any doctor-lead scenario highly, and only show a significant decrease in trust when a doctor is not presented as making the final decision, as in scenario 4.

Cluster 2, which leans the most towards following efficacy statistics, has the highest proportion of both medics and high-stakes decision makers. This might indicate that the cluster is composed principally of medical professionals, who are likely to rate themselves as more frequently making high-stakes decisions. This would also explain why this cluster has the lowest average self-reported AI knowledge compared to the whole survey population.

Clusters 0 and 3 are less easily identified, but might represent different subsets of the general population. Cluster 0 has a higher self-reported stake level, but a lower than average medic proportion, so might represent high-stakes decision makers from outside the medical field. Cluster 3 is very close to the average in all three of our profiling features, and is the largest, so could be said to represent the public, or the equivalent for our participant pool.

\section{Findings - Focus Group}

This section details the findings from thematic analysis of the transcript from the focus-group session, comprising several recurring themes from the discussion, and statements relevant to the results of the survey study. 

The pseudonymised transcript was divided into individual statements, which were independently systematically coded by two of the authors, by labeling with meaning and context. The coded statements were then collated and used to synthesise descriptive and analytical themes. The suggested themes were reviewed by independently by all authors, before meeting to discuss their implications, and to agree upon the central themes emerging from the data.    

Whilst the focus-group was designed to encourage discussion about the subject of trustworthy AI in any context, discussion tended to remain focused on a clinical context unless otherwise prompted.

Statements associated with each of the six participants are labeled as P1-P6. 


\subsection{Themes: Performance vs. Transparency}

\subsubsection{Performance of AI is Important}


\subsubsection{Transparency Less Important for Low-Stakes Contexts}\hfill\\

P1 [on AI in targeted advertising and social media] - ''Honestly, it doesn’t really bother me. If I'm getting targeted ads that are for me, you know I don’t want to get adverts for things that I don’t need in my life. So, it doesn’t bother me that much.''

[Quote in response from P5 suggesting there may be more issues with low-stakes scenarios as well regarding data protection]

[Quote on not knowing what app suggestions are doing]



\subsubsection{Transparency Required for Higher Stakes Contexts}\hfill\\

Several participants demonstrate awareness of the implications of opaque AI - a user cannot know how an algorithm or AI system can to a decision.

P6 outlines the problem well when asked about their feelings on targeted advertising and social media - "if whoever is using the algorithm doesn’t understand how it reaches those decisions then I guess it can reach the wrong decisions and no one will know and there is no way of catching that." 

P1 describes a scenario in which, for low stakes decisions, a lack of explainability and transparency can be beneficial through it's convenience.
"My phone knows what app I want to use before I even do. I open my phone and it says “do you want to use [App]?” and I'm like yeah I do, I don’t know how you know that but I do. So in that sense I think that it’s a really positive thing, and I think that it makes organisation a lot easier. But there are definitely scary parts to it". P1 acknowledges that a lack of transparency has "scary" aspects, despite having an overall positive sentiment towards it. 

P1 continues "ultimately, it decides what those apps are going to be and it may not be influenced by my decisions. It may be choosing those based on whatever it likes, I have no idea". There is a trade off here, where the user may be willing to relinquish control and autonomy in return for convenience and utility. 



subsubsection{Bias and Accountability}\hfill\\

P2 raises concerns around AI systems introducing or perpetuating social bias - "I think there was one [example] about insurance, where if your name was [X] then your insurance was multiple time higher than if your name was [Y] That is the fear side of it, the more we allow those sort of decisions to be made by people called [Y] then that’s great because all the [Y]'s get cheap insurance but that’s where my fear lies.


\subsubsection{The Importance of a Human in Clinical Pathways}




\subsubsection{}


\subsubsection{Human-AI Collaboration in the Diagnostic Pathway - AI as a Second Opinion}

Survey findings show that in general, participants ranked scenario (3/C) [black-box with human] higher than scenario D/4 [blackbox alone]. 
Participants of the focus-group discussed why they agreed or disagreed with this trend. One participant explained why they did not perceive that the human in scenario (3/C) improved the trustworthiness with respect to scenario 4/D without the human. 

56 (P6) “I definitely put [black-box] above black box with human”... 
“the human has no way of knowing whether what the black box did was right so why would it disagree with it, so it seems pointless having the human element in there”

Participant 5 responded to this point, and explained their reasoning for thinking that scenario C/3 was more trustworthy than scenario D/4. 

58 (P5) “a black box and human should be better just a black box because... [it's similar to] who wants to be a millionaire where you ask the audience... you get a bunch of statistics, and you can confirm your belief or not”. 
The participant makes the point that the doctor has their own assessment of the situation in addition to the assessment provided by the black-box.
The doctor could use the black-box output as a way of confirming their belief (a kind of second opinion), or to indicate where they may need to review their assessment. Further the participant suggests that the doctor may over time develop an intuition for the kinds of mistakes the black-box will make:
“if they have had past results and background then they’ll know roughly whether it’s accurate or not”.

Participant 6 clarifies the reasoning behind their decision by raising valid questions which highlight the ambiguity surrounding the relationship between the  human and AI system within the decision making process.

60-62 (P6)
“is it that the human and the black box makes a decision and they collate? The way I saw it was that the black box makes the decision and the human without any additional information makes a decision separately decided to accept it or not. “
“I guess in an ideal work you'd know that black box was taking in all the different information that it needed to. Especially I think in terms of minorities which have a history of not having that much data. So, I think that’s one element of having the human there that they could take that sort of information into account when maybe the black box doesn’t. “
The human and AI could fit into the wider decision making process in different places and in different ways, which could result in or address different biases.


\subsection{The Importance of Human Contact}


\section{Discussion}
Contribution we've made

%AD - from changes in trust and clustering

\subsubsection{Cooperation (AD), changes:}
\begin{itemize}
    \item AI experts like the AI cooperation scenarios most - 60\% have higher trust than in doctor
    \item If you like the black box, you also like the glass box, and very few in any population only like the black box
    \item AI experts have the smallest proportion liking the glass box and disliking the black box
    \item Disliking both glass box and black box follows AI technical level (descending): Public $\rightarrow$ Medics $\rightarrow$ AI
\end{itemize}

\subsubsection{Clustering (AD):}
\begin{itemize}
    \item Clustering isolates medics and AI researchers
    \item AI researchers follow what we'd expect: they trust AI the most, but weight towards transparency over efficacy, and don't like the black-box working solo
    \item Medics weight heavily towards efficacy, but still rate a doctor having the final decision higher than the black box by itself
    \item General population is split into low/high stakes
    \item High-stakes genpop are less scared of AI, and seem to weight efficacy and transparency equally important, but still want a human final decision
    \item Low-stakes genpop (the actual public?) are scared of AI, especially when its not transparent. They don't like the Terminator doctor, sorry Arnold.
\end{itemize}

%ID - conclusions from Ranking and Literature

\subsubsection{Ranking (ID)}
\begin{itemize}
    \item BB least trusted across demographics
    \item GB+DR most trusted across demographics
    \item the second least trusted scenario was Dr on their own
    \item DR on their own second most trusted scenario
    \item higher stakes decision makers trust the Dr less
    \item medium+high AI expertise trust BB more than the low expertise
    \item from Focus group, dislike of BB has more to do with lack of empathy in the communication not if it is interpretable, transparent or not
    \item from FG high performance is important for BB
\end{itemize}

%MC - from focus group
\subsubsection{focus group (MC):}
\begin{itemize}
    \item preference to doctor or human relaying information
    \item full disclosure of any AI used in a system
    \item performance of AI important
    \item transparancy of AI important for high stakes
    \item low stakes transparancy less important
    \item confusing over who is accountable for AI decsions
    \item second opinion from AI seen as useful over doctor on own
    \item trust issues around data privacy
\end{itemize}

\subsubsection{Overall themes/contrasts (AD):}

\begin{itemize}
    \item Preference for AI cooperation increases with technical expertise level
    \item Human-in-the-loop is always preferred - backed up by focus group
    \item Transparency is also important - across all cohorts no one really prefers the higher performance BB coop to more transparent GB coop
    \item Issues of trust from focus group about accountability
\end{itemize}




\section{Conclusion}



%%
%% The acknowledgments section is defined using the "acks" environment
%% (and NOT an unnumbered section). This ensures the proper
%% identification of the section in the article metadata, and the
%% consistent spelling of the heading.
\begin{acks}
Acknowledgments
\end{acks}

%%
%% The next two lines define the bibliography style to be used, and
%% the bibliography file.
\newpage
\bibliographystyle{ACM-Reference-Format}
\bibliography{elf}

%%
%% If your work has an appendix, this is the place to put it.
\appendix

\section{Optional Supplementary Material}

\subsection{Study Ethics}

% DC - Need to go through and format, I've been working in a google doc
% How do I add line space after subsubheading?
\subsubsection{Organisation and Funding}


This work was organised and funded by The University of Bristol School of Computer Science, Electrical and Electronic Engineering, and Engineering Maths.

Investigators are students enrolled on the “Interactive Artificial Intelligence CDT” at the University of Bristol, and are funded by UKRI Studentships.

This work was completed as part of a University course “Interaction Design COMSM0087”, led by Dr Paul Marshall.


\subsubsection{Ethical Approval} 


Ethical approval for study methods described in Section [?] of the report was granted by University of Bristol Faculty of Engineering Research Ethics committee prior to study recruitment.

The study has been reviewed by the unit director, Dr Paul Marshall.

The risks associated with participating in the study were assessed as low likelihood and low severity. 

\subsubsection{Incentive}  


At the recruitment stage, prospective participants were told that there would be a monetary incentive for participation.

For participation in the survey, this was the option to be entered into a raffle to win one of four Amazon gift vouchers worth £25 each.

To facilitate this, participants could opt in to the raffle by providing an email address, which was stored securely on a University server. 

Email addresses were stored separately from survey responses to prevent any reassociated. 

At the time that the survey was closed to new participants, four email addresses were selected at random to receive the vouchers.

Once the selected participants had confirmed they received the vouchers, all email addresses were promptly deleted. 



\subsubsection{Privacy and Data Management}  


Survey:
No personal identifiable information was collected during the survey phase of this work.

Survey data was collected via Microsoft Forms, and stored on a secure server hosted by the University of Bristol.

Participants had the option of providing an email address to opt into a raffle to win a monetary prize. 
This email address was not associated with their survey response, and was used only to send the prize to the relevant participants. 
Email addresses were stored on a secure University of Bristol server and deleted after prizes were allocated.  

Focus Group:
The focus-group was conducted remotely via Zoom online video conferencing. 

The focus-group was recorded for the purpose of transcribing the discussion, and was deleted when the audio had been transcribed. 

All audio transcriptions were pseudonymised prior to analysis.

All data gathered for this work (survey responses, email addresses, focus-group audio, transcript) was stored on a secure University of Bristol server. 

\subsubsection{Recruitment:}  


The total number of survey participants was [XXXX] at the time of closing.
Survey participants were recruited by:
Direct contact
Sharing the survey with personal contacts
Mailbases 
Survey was sent to the directors of the following University programs to be  distributed to the students:
[Which programmes?]
The survey was posted in a medical physics and engineering special interest mailing group: JISCMAIL Medical-Physics-Engineering 
Social media
The survey was distributed through personal Twitter accounts (resulted in about 3 participants) as well as 5300 followers dogs of Twitter account with followers from all over the world and different backgrounds which resulted in over 100 participants.
Participation was open to all members of the public, however recruitment was targeted towards three groups: healthcare professionals, AI/ML specialists, and general public. The studies population is not therefore reflective of a cross-section of the general public or specific demographic groups. 
No demographic information relating to protected characteristics was collected. 
The only objective information requested from participants was whether or not they work as a healthcare professional. 
Six focus-group participants were recruited from personal contacts. 

\subsubsection{Informed Consent Survey}  


Written consent was obtained for the survey study…
A participant information sheet was presented at the start of the survey. Participants were required to confirm that they have read the information sheet before progressing with the survey to ensure informed consent as much as was reasonably practicable. 
I.e. “By clicking "Next" you confirm that you've read the full information about this study and consent to participate.”
The participant information sheet included:
Background about the project and project organisers.
The purpose of the study
Reason participants have been recruited. 
Eligibility criteria
Details of participation:
Emphasis that participation is voluntary
Emphasis that participant can choose to withdraw from the study (relevant to focus group).
Description of how data will be managed and used.
Details of incentive.
Details of confidentiality. 
Emphasis that no personally identifiable information is recorded in the survey. 
Right to withdrawal.
Contact details for asking questions. 
Intended outcomes of the research project.
Eligibility criteria i.e. requirements for participation as specified in the information sheet include:
Participant is over 18 years old
Participant has internet access to participate in online data collection

 
\subsubsection{Informed Consent Focus-Group}  


Participants were sent an information sheet directly, and asked to confirm they had read and understood the provided information prior to the event.

At the start of the focus group session, participants were given the opportunity to say if they had not read or understood the provided information, or if they had any concerns they wanted to raise. 

\subsubsection{Other}  



%Check notes from other group
%Check any other comments/document related to ethics
%So far, I've mostly just looked at the ethical approval document and the details from our information sheet.


% DC - Need to go through and format, I've been working in a google doc
% How do I add line space after subsubheading?


\subsection{Part Two}  




\end{document}
\endinput
%%
%% End of file.
