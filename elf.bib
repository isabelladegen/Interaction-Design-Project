@article{McInnes2017,
   abstract = {HDBSCAN: Hierarchical Density-Based Spatial Clustering of Applications with Noise (Campello, Moulavi, and Sander 2013), (Campello et al. 2015). Performs DBSCAN over varying epsilon values and integrates the result to find a clustering that gives the best stability over epsilon. This allows HDBSCAN to find clusters of varying densities (unlike DBSCAN), and be more robust to parameter selection. The library also includes support for Robust Single Linkage clustering (Chaudhuri et al. 2014), (Chaudhuri and Dasgupta 2010), GLOSH outlier detection (Campello et al. 2015), and tools for visualizing and exploring cluster structures. Finally support for prediction and soft clustering is also available.},
   author = {Leland McInnes and John Healy and Steve Astels},
   doi = {10.21105/JOSS.00205},
   issue = {11},
   journal = {The Journal of Open Source Software},
   month = {3},
   pages = {205},
   publisher = {The Open Journal},
   title = {hdbscan: Hierarchical density based clustering},
   volume = {2},
   year = {2017},
}
@article{McInnes2018,
   abstract = {UMAP (Uniform Manifold Approximation and Projection) is a novel manifold
learning technique for dimension reduction. UMAP is constructed from a
theoretical framework based in Riemannian geometry and algebraic topology. The
result is a practical scalable algorithm that applies to real world data. The
UMAP algorithm is competitive with t-SNE for visualization quality, and
arguably preserves more of the global structure with superior run time
performance. Furthermore, UMAP has no computational restrictions on embedding
dimension, making it viable as a general purpose dimension reduction technique
for machine learning.},
   author = {Leland McInnes and John Healy and James Melville},
   doi = {10.48550/arxiv.1802.03426},
   month = {2},
   title = {UMAP: Uniform Manifold Approximation and Projection for Dimension Reduction},
   url = {https://arxiv.org/abs/1802.03426v3},
   year = {2018},
}
@article{Felzmann2019,
   abstract = {Transparency is now a fundamental principle for data processing under the General Data Protection Regulation. We explore what this requirement entails for artificial intelligence and automated deci...},
   author = {Heike Felzmann and Eduard Fosch Villaronga and Christoph Lutz and Aurelia Tamò-Larrieux},
   doi = {10.1177/2053951719860542},
   issn = {20539517},
   issue = {1},
   journal = {https://doi.org/10.1177/2053951719860542},
   keywords = {Artificial intelligence,HRI,automated decision-making,ethics,general data protection regulation,human– computer interaction,transparency},
   month = {6},
   publisher = {SAGE PublicationsSage UK: London, England},
   title = {Transparency you can trust: Transparency requirements for artificial intelligence between legal norms and contextual concerns:},
   volume = {6},
   url = {https://journals.sagepub.com/doi/full/10.1177/2053951719860542},
   year = {2019},
}
@article{Diaz2021,
   abstract = {Purpose: To assess current perceptions, practices and education needs pertaining to artificial intelligence (AI) in the medical physics field. Methods: A web-based survey was distributed to the European Federation of Organizations for Medical Physics (EFOMP) through social media and email membership list. The survey included questions about education, personal knowledge, needs, research and professionalism around AI in medical physics. Demographics information were also collected. Responses were stratified and analysed by gender, type of institution and years of experience in medical physics. Statistical significance (p<0.05) was assessed using paired t-test. Results: 219 people from 31 countries took part in the survey. 81% (n = 177) of participants agreed that AI will improve the daily work of Medical Physics Experts (MPEs) and 88% (n = 193) of respondents expressed the need for MPEs of specific training on AI. The average level of AI knowledge among participants was 2.3 ± 1.0 (mean ± standard deviation) in a 1-to-5 scale and 96% (n = 210) of participants showed interest in improving their AI skills. A significantly lower AI knowledge was observed for female participants (2.0 ± 1.0), compared to male responders (2.4 ± 1.0). 64% of participants indicated that they are not involved in AI projects. The percentage of female leading AI projects was significantly lower than the male counterparts (3% vs 19%). Conclusions: AI was perceived as a positive resource to support MPEs in their daily tasks. Participants demonstrated a strong interest in improving their current AI-related skills, enhancing the need for dedicated training for MPEs.},
   author = {Oliver Diaz and Gabriele Guidi and Oleksandra Ivashchenko and Niall Colgan and Federica Zanca},
   doi = {10.1016/J.EJMP.2020.11.037},
   issn = {1120-1797},
   journal = {Physica Medica},
   keywords = {Artificial intelligence,Medical physics,Medical technology,Survey},
   month = {1},
   pages = {141-146},
   pmid = {33453506},
   publisher = {Elsevier},
   title = {Artificial intelligence in the medical physics community: An international survey},
   volume = {81},
   year = {2021},
}
@generic{Markus2021,
   abstract = {Artificial intelligence (AI) has huge potential to improve the health and well-being of people, but adoption in clinical practice is still limited. Lack of transparency is identified as one of the main barriers to implementation, as clinicians should be confident the AI system can be trusted. Explainable AI has the potential to overcome this issue and can be a step towards trustworthy AI. In this paper we review the recent literature to provide guidance to researchers and practitioners on the design of explainable AI systems for the health-care domain and contribute to formalization of the field of explainable AI. We argue the reason to demand explainability determines what should be explained as this determines the relative importance of the properties of explainability (i.e. interpretability and fidelity). Based on this, we propose a framework to guide the choice between classes of explainable AI methods (explainable modelling versus post-hoc explanation; model-based, attribution-based, or example-based explanations; global and local explanations). Furthermore, we find that quantitative evaluation metrics, which are important for objective standardized evaluation, are still lacking for some properties (e.g. clarity) and types of explanations (e.g. example-based methods). We conclude that explainable modelling can contribute to trustworthy AI, but the benefits of explainability still need to be proven in practice and complementary measures might be needed to create trustworthy AI in health care (e.g. reporting data quality, performing extensive (external) validation, and regulation).},
   author = {Aniek F. Markus and Jan A. Kors and Peter R. Rijnbeek},
   doi = {10.1016/j.jbi.2020.103655},
   issn = {15320464},
   journal = {Journal of Biomedical Informatics},
   keywords = {Explainable artificial intelligence,Explainable modelling,Interpretability,Post-hoc explanation,Trustworthy artificial intelligence},
   month = {1},
   pmid = {33309898},
   publisher = {Academic Press Inc.},
   title = {The role of explainability in creating trustworthy artificial intelligence for health care: A comprehensive survey of the terminology, design choices, and evaluation strategies},
   volume = {113},
   year = {2021},
}
@article{Ehsan2020,
   abstract = {Explanations—a form of post-hoc interpretability—play an instrumental role in making systems accessible as AI continues to proliferate complex and sensitive sociotechnical systems. In this paper, we introduce Human-centered Explainable AI (HCXAI) as an approach that puts the human at the center of technology design. It develops a holistic understanding of “who” the human is by considering the interplay of values, interpersonal dynamics, and the socially situated nature of AI systems. In particular, we advocate for a reflective sociotechnical approach. We illustrate HCXAI through a case study of an explanation system for non-technical end-users that shows how technical advancements and the understanding of human factors co-evolve. Building on the case study, we lay out open research questions pertaining to further refining our understanding of “who” the human is and extending beyond 1-to-1 human-computer interactions. Finally, we propose that a reflective HCXAI paradigm—mediated through the perspective of Critical Technical Practice and supplemented with strategies from HCI, such as value-sensitive design and participatory design—not only helps us understand our intellectual blind spots, but it can also open up new design and research spaces.},
   author = {Upol Ehsan and Mark O. Riedl},
   doi = {10.1007/978-3-030-60117-1_33},
   isbn = {9783030601164},
   issn = {16113349},
   journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   keywords = {Artificial intelligence,Critical technical practice,Explainable AI,Human-centered computing,Interpretability,Machine learning,Rationale generation,Sociotechnical,User perception},
   month = {7},
   pages = {449-466},
   publisher = {Springer, Cham},
   title = {Human-Centered Explainable AI: Towards a Reflective Sociotechnical Approach},
   volume = {12424 LNCS},
   url = {https://link.springer.com/chapter/10.1007/978-3-030-60117-1_33},
   year = {2020},
}
@article{Nazar2021,
   abstract = {Artificial intelligence (AI) is one of the emerging technologies. In recent decades, artificial intelligence (AI) has gained widespread acceptance in a variety of fields, including virtual support, healthcare, and security. Human-Computer Interaction (HCI) is a field that has been combining AI and human-computer engagement over the past several years in order to create an interactive intelligent system for user interaction. AI, in conjunction with HCI, is being used in a variety of fields by employing various algorithms and employing HCI to provide transparency to the user, allowing them to trust the machine. The comprehensive examination of both the areas of AI and HCI, as well as their subfields, has been explored in this work. The main goal of this article was to discover a point of intersection between the two fields. The understanding of Explainable Artificial Intelligence (XAI), which is a linking point of HCI and XAI, was gained through a literature review conducted in this research. The literature survey encompassed themes identified in the literature (such as XAI and its areas, major XAI aims, and XAI problems and challenges). The study's other major focus was on the use of AI, HCI, and XAI in healthcare. The poll also addressed the shortcomings in XAI in healthcare, as well as the field's future potential. As a result, the literature indicates that XAI in healthcare is still a novel subject that has to be explored more in the future.},
   author = {Mobeen Nazar and Muhammad Mansoor Alam and Eiad Yafi and Mazliham Mohd Su'Ud},
   doi = {10.1109/ACCESS.2021.3127881},
   issn = {21693536},
   journal = {IEEE Access},
   keywords = {Artificial intelligence,deep learning,explainable artificial intelligence,healthcare,human-centered design,human-computer interaction,machine learning,usability,user-centered design},
   pages = {153316-153348},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {A Systematic Review of Human-Computer Interaction and Explainable Artificial Intelligence in Healthcare with Artificial Intelligence Techniques},
   volume = {9},
   year = {2021},
}
@article{Bansal2020,
   abstract = {Many researchers motivate explainable AI with studies showing that human-AI
team performance on decision-making tasks improves when the AI explains its
recommendations. However, prior studies observed improvements from explanations
only when the AI, alone, outperformed both the human and the best team. Can
explanations help lead to complementary performance, where team accuracy is
higher than either the human or the AI working solo? We conduct mixed-method
user studies on three datasets, where an AI with accuracy comparable to humans
helps participants solve a task (explaining itself in some conditions). While
we observed complementary improvements from AI augmentation, they were not
increased by explanations. Rather, explanations increased the chance that
humans will accept the AI's recommendation, regardless of its correctness. Our
result poses new challenges for human-centered AI: Can we develop explanatory
approaches that encourage appropriate trust in AI, and therefore help generate
(or improve) complementary performance?},
   author = {Gagan Bansal and Tongshuang Wu and Joyce Zhou and Raymond Fok and Besmira Nushi and Ece Kamar and Marco Tulio Ribeiro and Daniel S. Weld},
   doi = {10.48550/arxiv.2006.14779},
   isbn = {9781450380966},
   journal = {Conference on Human Factors in Computing Systems - Proceedings},
   keywords = {Augmented intelligence,Explainable ai,Human-ai teams},
   month = {6},
   pages = {16},
   publisher = {Association for Computing Machinery},
   title = {Does the Whole Exceed its Parts? The Effect of AI Explanations on Complementary Team Performance},
   url = {https://arxiv.org/abs/2006.14779v3},
   year = {2020},
}
@article{Hancock2011,
   abstract = {Objective: We evaluate and quantify the effects of human, robot, and environmental factors on perceived trust in human-robot interaction (HRI).Background: To date, reviews of trust in HRI have been qualitative or descriptive. Our quantitative review provides a fundamental empirical foundation to advance both theory and practice.Method: Meta-analytic methods were applied to the available literature on trust and HRI. A total of 29 empirical studies were collected, of which 10 met the selection criteria for correlational analysis and 11 for experimental analysis. These studies provided 69 correlational and 47 experimental effect sizes.Results: The overall correlational effect size for trust was r- = +0.26, with an experimental effect size of d- = +0.71. The effects of human, robot, and environmental characteristics were examined with an especial evaluation of the robot dimensions of performance and attribute-based factors. The robot performance and attributes were the largest contributors to the development of trust in HRI. Environmental factors played only a moderate role.Conclusion: Factors related to the robot itself, specifically, its performance, had the greatest current association with trust, and environmental factors were moderately associated. There was little evidence for effects of human-related factors.Application: The findings provide quantitative estimates of human, robot, and environmental factors influencing HRI trust. Specifically, the current summary provides effect size estimates that are useful in establishing design and training guidelines with reference to robot-related factors of HRI trust. Furthermore, results indicate that improper trust calibration may be mitigated by the manipulation of robot design. However, many future research needs are identified. © 2011, Human Factors and Ergonomics Society.},
   author = {Peter A. Hancock and Deborah R. Billings and Kristin E. Schaefer and Jessie Y.C. Chen and Ewart J. De Visser and Raja Parasuraman},
   doi = {10.1177/0018720811417254},
   issn = {00187208},
   issue = {5},
   journal = {Human Factors},
   keywords = {human-robot team,robotics,trust,trust development},
   month = {10},
   pages = {517-527},
   pmid = {22046724},
   publisher = {SAGE PublicationsSage CA: Los Angeles, CA},
   title = {A meta-analysis of factors affecting trust in human-robot interaction},
   volume = {53},
   url = {https://journals-sagepub-com.bris.idm.oclc.org/doi/10.1177/0018720811417254},
   year = {2011},
}
@article{Toreini2020,
   abstract = {To design and develop AI-based systems that users and the larger public can justifiably trust, one needs to understand how machine learning technologies impact trust. To guide the design and implementation of trusted AI-based systems, this paper provides a systematic approach to relate considerations about trust from the social sciences to trustworthiness technologies proposed for AI-based services and products. We start from the ABI+ (Ability, Benevolence, Integrity, Predictability) framework augmented with a recently proposed mapping of ABI+ on qualities of technologies that support trust. We consider four categories of trustworthiness technologies for machine learning, namely these for Fairness, Explainability, Au-ditability and Safety (FEAS) and discuss if and how these support the required qualities. Moreover, trust can be impacted throughout the life cycle of AI-based systems, and we therefore introduce the concept of Chain of Trust to discuss trustworthiness technologies in all stages of the life cycle. In so doing we establish the ways in which machine learning technologies support trusted AI-based systems. Finally, FEAS has obvious relations with known frameworks and therefore we relate FEAS to a variety of international 'princi-pled AI' policy and technology frameworks that have emerged in recent years. CCS CONCEPTS • Applied computing → Sociology; • Social and professional topics → Computing / technology policy; • Security and privacy → Human and societal aspects of security and privacy; • Computing methodologies → Artificial intelligence; Machine learning .},
   author = {Ehsan Toreini and Mhairi Aitken and Karen Elliott and Carlos Gonzalez Zelaya and Kovila Coopamootoo and Aad van Moorsel},
   city = {New York, NY, USA},
   doi = {10.1145/3351095},
   isbn = {9781450369367},
   journal = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
   keywords = {artificial intelligence,machine learning,trust,trustworthiness},
   publisher = {ACM},
   title = {The relationship between trust in AI and trustworthy machine learning technologies},
   url = {https://doi.org/10.1145/3351095.3372834},
}
@article{Chen2021,
   abstract = {In this paper, I will identify two problems of trust in an AI-relevant context: a theoretical problem and a practical one. I will identify and address a number of skeptical challenges to an AI-relevant theory of trust. In addition, I will identify what I shall term the ‘scope challenge’, which I take to hold for any AI-relevant theory (or collection of theories) of trust that purports to be representationally adequate to the multifarious forms of trust and AI. Thereafter, I will suggest how trust-engineering, a position that is intermediate between the modified pure rational-choice account and an account that gives rise to trustworthy AI, might allow us to address the practical problem of trust, before identifying and critically evaluating two candidate trust-engineering approaches.},
   author = {Melvin Chen},
   doi = {10.1007/S13347-021-00465-4/FIGURES/1},
   issn = {22105441},
   issue = {4},
   journal = {Philosophy and Technology},
   keywords = {Cunning of trust,Problems of trust,Trust,Trust-engineering,Trustworthiness},
   month = {12},
   pages = {1429-1447},
   publisher = {Springer Science and Business Media B.V.},
   title = {Trust and Trust-Engineering in Artificial Intelligence Research: Theory and Praxis},
   volume = {34},
   url = {https://link.springer.com/article/10.1007/s13347-021-00465-4},
   year = {2021},
}
@article{Ferrario2020,
   abstract = {Real engines of the artificial intelligence (AI) revolution, machine learning (ML) models, and algorithms are embedded nowadays in many services and products around us. As a society, we argue it is now necessary to transition into a phronetic paradigm focused on the ethical dilemmas stemming from the conception and application of AIs to define actionable recommendations as well as normative solutions. However, both academic research and society-driven initiatives are still quite far from clearly defining a solid program of study and intervention. In this contribution, we will focus on selected ethical investigations around AI by proposing an incremental model of trust that can be applied to both human-human and human-AI interactions. Starting with a quick overview of the existing accounts of trust, with special attention to Taddeo’s concept of “e-trust,” we will discuss all the components of the proposed model and the reasons to trust in human-AI interactions in an example of relevance for business organizations. We end this contribution with an analysis of the epistemic and pragmatic reasons of trust in human-AI interactions and with a discussion of kinds of normativity in trustworthiness of AIs.},
   author = {Andrea Ferrario and Michele Loi and Eleonora Viganò},
   doi = {10.1007/S13347-019-00378-3/TABLES/3},
   issn = {22105441},
   issue = {3},
   journal = {Philosophy and Technology},
   keywords = {Artificial intelligence (AI),E-trust,Trust,Trustworthiness},
   month = {9},
   pages = {523-539},
   publisher = {Springer},
   title = {In AI We Trust Incrementally: a Multi-layer Model of Trust to Analyze Human-Artificial Intelligence Interactions},
   volume = {33},
   url = {https://link.springer.com/article/10.1007/s13347-019-00378-3},
   year = {2020},
}
@article{Chandrasekaran1989,
   author = {B. Chandrasekaran and Michael C. Tanner and John R. Josephson},
   doi = {10.1109/64.21896},
   issn = {08859000},
   issue = {1},
   journal = {IEEE Expert-Intelligent Systems and their Applications},
   pages = {9-15},
   title = {Explaining Control Strategies in Problem Solving},
   volume = {4},
   year = {1989},
}
@article{Hoff2015,
   abstract = {Objective: We systematically review recent empirical research on factors that influence trust in automation to present a three-layered trust model that synthesizes existing knowledge. Background: Much of the existing research on factors that guide human-automation interaction is centered around trust, a variable that often determines the willingness of human operators to rely on automation. Studies have utilized a variety of different automated systems in diverse experimental paradigms to identify factors that impact operators trust. Method: We performed a systematic review of empirical research on trust in automation from January 2002 to June 2013. Papers were deemed eligible only if they reported the results of a human-subjects experiment in which humans interacted with an automated system in order to achieve a goal. Additionally, a relationship between trust (or a trust-related behavior) and another variable had to be measured. All together, 101 total papers, containing 127 eligible studies, were included in the review. Results: Our analysis revealed three layers of variability in humanautomation trust (dispositional trust, situational trust, and learned trust), which we organize into a model. We propose design recommendations for creating trustworthy automation and identify environmental conditions that can affect the strength of the relationship between trust and reliance. Future research directions are also discussed for each layer of trust. Conclusion: Our three-layered trust model provides a new lens for conceptualizing the variability of trust in automation. Its structure can be applied to help guide future research and develop training interventions and design procedures that encourage appropriate trust.},
   author = {Kevin Anthony Hoff and Masooda Bashir},
   doi = {10.1177/0018720814547570},
   issn = {15478181},
   issue = {3},
   journal = {Human Factors},
   keywords = {automated system,human-automation interaction,reliance,trust formation,trust in automation},
   month = {5},
   pages = {407-434},
   pmid = {25875432},
   publisher = {SAGE Publications Inc.},
   title = {Trust in automation: Integrating empirical evidence on factors that influence trust},
   volume = {57},
   url = {https://journals.sagepub.com/doi/abs/10.1177/0018720814547570?journalCode=hfsa},
   year = {2015},
}   
@article{Lee2008,
   abstract = {Objective: This paper considers the influence of "Humans and Automation: Use, Misuse, Disuse, Abuse" and examines how it relates to the evolving issue of human-automation interaction. Background: Automation presents important practical challenges that can dramatically affect satisfaction, performance, and safety; philosophical challenges also arise as automation changes the nature of work and human cognition. Method: Papers cited by and citing "Humans and Automation" were reviewed to identify enduring and emerging themes in human-automation research. Results: "Humans and Automation" emerges as an important node in the network of automation-related papers, citing many and being cited by many recent influential automation-related papers. In their article, Parasuraman and Riley (1997) integrated previous research and identified differing expectations across designers, managers, and operators regarding the need to support operators as a source of automation problems. They also foresaw and inspired research that addresses problems of overreliance and underreliance on automation. Conclusion: This pivotal article and associated research show that even though automation seems to relieve people of tasks, automation requires more, not less, attention to training, interface design, and interaction design. The original article also alludes to the emergence of vicious cycles and dysfunctional meta-control. These problems reflect the coevolution of automation and humans, in which both adapt to the responses of the other. Application: Understanding this coevolution has important philosophical implications for the nature of human cognition and practical implications for satisfaction, performance, and safety. Copyright © 2008, Human Factors and Ergonomics Society.},
   author = {John D. Lee},
   doi = {10.1518/001872008X288547},
   issn = {0018-7208},
   issue = {3},
   journal = {Human factors},
   keywords = {Ergonomics*,Humans,John D Lee,MEDLINE,Man-Machine Systems*,NCBI,NIH,NLM,National Center for Biotechnology Information,National Institutes of Health,National Library of Medicine,PubMed Abstract,Review,doi:10.1518/001872008X288547,pmid:18689046},
   month = {6},
   pages = {404-410},
   pmid = {18689046},
   publisher = {Hum Factors},
   title = {Review of a pivotal Human Factors article: "Humans and automation: use, misuse, disuse, abuse"},
   volume = {50},
   url = {https://pubmed.ncbi.nlm.nih.gov/18689046/},
   year = {2008},
}
@article{Muir1987,
   abstract = {A problem in the design of decision aids is how to design them so that decision makers will trust them and therefore use them appropriately. This problem is approached in this paper by taking models of trust between humans as a starting point, and extending these to the human-machine relationship. A definition and model of human-machine trust are proposed, and the dynamics of trust between humans and machines are examined. Based upon this analysis, recommendations are made for calibrating users' trust in decision aids. © 1987, Academic Press Limited. All rights reserved.},
   author = {Bonnie M. Muir},
   doi = {10.1016/S0020-7373(87)80013-5},
   issn = {00207373},
   issue = {5-6},
   journal = {International Journal of Man-Machine Studies},
   pages = {527-539},
   title = {Trust between humans and machines, and the design of decision aids},
   volume = {27},
   year = {1987},
}
@article{Miller2019,
   abstract = {There has been a recent resurgence in the area of explainable artificial intelligence as researchers and practitioners seek to provide more transparency to their algorithms. Much of this research is focused on explicitly explaining decisions or actions to a human observer, and it should not be controversial to say that looking at how humans explain to each other can serve as a useful starting point for explanation in artificial intelligence. However, it is fair to say that most work in explainable artificial intelligence uses only the researchers' intuition of what constitutes a ‘good’ explanation. There exist vast and valuable bodies of research in philosophy, psychology, and cognitive science of how people define, generate, select, evaluate, and present explanations, which argues that people employ certain cognitive biases and social expectations to the explanation process. This paper argues that the field of explainable artificial intelligence can build on this existing research, and reviews relevant papers from philosophy, cognitive psychology/science, and social psychology, which study these topics. It draws out some important findings, and discusses ways that these can be infused with work on explainable artificial intelligence.},
   author = {Tim Miller},
   doi = {10.1016/J.ARTINT.2018.07.007},
   issn = {00043702},
   journal = {Artificial Intelligence},
   keywords = {Explainability,Explainable AI,Explanation,Interpretability,Transparency},
   month = {2},
   pages = {1-38},
   publisher = {Elsevier B.V.},
   title = {Explanation in artificial intelligence: Insights from the social sciences},
   volume = {267},
   url = {www.elsevier.com/locate/artint},
   year = {2019},
}
%don't see how the below is relevant?
@article{Reilly2002,
   abstract = {Context Emergency department (ED) physicians often are uncertain about where in the hospital to triage patients with suspected acute cardiac ischemia. Many patients are triaged unnecessarily to intensive or intermediate cardiac care units.

Objective To determine whether use of a clinical decision rule improves physicians' hospital triage decisions for patients with suspected acute cardiac ischemia.

Design and Setting Prospective before-after impact analysis conducted at a large, urban, US public hospital.

Participants Consecutive patients admitted from the ED with suspected acute cardiac ischemia during 2 periods: preintervention group (n = 207 patients enrolled in March 1997) and intervention group (n = 1008 patients enrolled in August-November 1999).

Intervention An adaptation of a previously validated clinical decision rule was adopted as the standard of care in the ED after a 3-month period of pilot testing and training. The rule predicts major cardiac complications within 72 hours after evaluation in the ED and stratifies patients' risk of major complications into 4 groups—high, moderate, low, and very low—according to electrocardiographic findings and presence or absence of 3 clinical predictors in the ED.

Main Outcome Measures Safety of physicians' triage decisions, defined as the proportion of patients with major cardiac complications who were admitted to inpatient cardiac care beds (coronary care unit or inpatient telemetry unit); efficiency of decisions, defined as the proportion of patients without major complications who were triaged to an ED observation unit or an unmonitored ward.

Results By intention-to-treat analysis, efficiency was higher in the intervention group (36%) than the preintervention group (21%) (difference, 15%; 95% confidence interval [CI], 8%-21%; P<.001). Safety was not significantly different (94% in the intervention group vs 89%; difference, 5%; 95% CI, −11% to 39%; P = .57). Subgroup analysis of intervention-group patients showed higher efficiency when physicians actually used the decision rule (38% vs 27%; difference, 11%; 95% CI, 3%-18%; P = .01). Improved efficiency was explained solely by different triage decisions for very low-risk patients. Most surveyed physicians (16/19 [84%]) believed that the decision rule improved patient care.},
   author = {Brendan M. Reilly},
   doi = {10.1001/jama.288.3.342},
   issn = {0098-7484},
   issue = {3},
   journal = {JAMA},
   month = {7},
   note = {They find that a clinical decision making rule outperformed doctors (I think)},
   pages = {342},
   title = {Impact of a Clinical Decision Rule on Hospital Triage of Patients With Suspected Acute Cardiac Ischemia in the Emergency Department},
   volume = {288},
   year = {2002},
}
@article{Gilpin2019,
   abstract = {There has recently been a surge of work in explanatory artificial intelligence (XAI). This research area tackles the important problem that complex machines and algorithms often cannot provide insights into their behavior and thought processes. XAI allows users and parts of the internal system to be more transparent, providing explanations of their decisions in some level of detail. These explanations are important to ensure algorithmic fairness, identify potential bias/problems in the training data, and to ensure that the algorithms perform as expected. However, explanations produced by these systems is neither standardized nor systematically assessed. In an effort to create best practices and identify open challenges, we describe foundational concepts of explainability and show how they can be used to classify existing literature. We discuss why current approaches to explanatory methods especially for deep neural networks are insufficient. Finally, based on our survey, we conclude with suggested future research directions for explanatory artificial intelligence.},
   author = {Leilani H. Gilpin and David Bau and Ben Z. Yuan and Ayesha Bajwa and Michael Specter and Lalana Kagal},
   doi = {10.1109/DSAA.2018.00018},
   isbn = {9781538650905},
   journal = {Proceedings - 2018 IEEE 5th International Conference on Data Science and Advanced Analytics, DSAA 2018},
   keywords = {Deep learning and deep analytics,Fairness and transparency in data science,Machine learning theories,Models and systems},
   month = {1},
   pages = {80-89},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {Explaining explanations: An overview of interpretability of machine learning},
   year = {2019},
}
@article{Vilone2020,
   abstract = {Explainable Artificial Intelligence (XAI) has experienced a significant
growth over the last few years. This is due to the widespread application of
machine learning, particularly deep learning, that has led to the development
of highly accurate models but lack explainability and interpretability. A
plethora of methods to tackle this problem have been proposed, developed and
tested. This systematic review contributes to the body of knowledge by
clustering these methods with a hierarchical classification system with four
main clusters: review articles, theories and notions, methods and their
evaluation. It also summarises the state-of-the-art in XAI and recommends
future research directions.},
   author = {Giulia Vilone and Luca Longo},
   doi = {10.48550/arxiv.2006.00093},
   month = {5},
   title = {Explainable Artificial Intelligence: a Systematic Review},
   url = {https://arxiv.org/abs/2006.00093},
   year = {2020},
}
%not sure how to include this one
@article{Sokol2020,
   abstract = {Background Predictive systems, in particular machine learning algorithms, can take important, and sometimes legally binding, decisions about our everyday life. In most cases, however, these systems and decisions are neither regulated nor certified. Given the …},
   author = {Kacper Sokol and Alexander Hepburn and Rafael Poyiadzi and Matthew Clifford and Raul Santos-Rodriguez and Peter Flach},
   doi = {10.21105/joss.01904},
   issue = {49},
   journal = {Journal of Open Source Software},
   month = {5},
   pages = {1904},
   publisher = {The Open Journal},
   title = {FAT Forensics: A Python Toolbox for Implementing and Deploying Fairness, Accountability and Transparency Algorithms in Predictive Systems},
   volume = {5},
   year = {2020},
}
@article{Schwalbe2021,
   abstract = {In the meantime, a wide variety of terminologies, motivations, approaches and
evaluation criteria have been developed within the research field of
explainable artificial intelligence (XAI). With the amount of XAI methods
vastly growing, a taxonomy of methods is needed by researchers as well as
practitioners: To grasp the breadth of the topic, compare methods, and to
select the right XAI method based on traits required by a specific use-case
context. In the literature many taxonomies for XAI methods of varying level of
detail and depth can be found. While they often have a different focus, they
also exhibit many points of overlap. This paper unifies these efforts, and
provides a taxonomy of XAI methods that is complete with respect to notions
present in the current state-of-research. In a structured literature analysis
and meta-study we identified and reviewed more than 50 of the most cited and
current surveys on XAI methods, metrics, and method traits. After summarizing
them in a survey of surveys, we merge terminologies and concepts of the
articles into a unified structured taxonomy. Single concepts therein are
illustrated by in total more than 50 diverse selected example methods, which we
categorize accordingly. The taxonomy may serve both beginners, researchers, and
practitioners as a reference and wide-ranging overview on XAI method traits and
aspects. Hence, it provides foundations for targeted, use-case-oriented, and
context-sensitive future research.},
   author = {Gesina Schwalbe and Bettina Finzel},
   isbn = {2105.07190v2},
   keywords = {Analysis ·,Artificial,Explainable,Intelligence ·,Interpretability ·,Meta-,Review,Survey-of-surveys ·,Tax-onomy ·},
   month = {5},
   title = {A Comprehensive Taxonomy for Explainable Artificial Intelligence: A Systematic Survey of Surveys on Methods and Concepts},
   url = {https://arxiv.org/abs/2105.07190v2},
   year = {2021},
}
@article{Ras2020,
   abstract = {Deep neural networks (DNNs) have become a proven and indispensable machine
learning tool. As a black-box model, it remains difficult to diagnose what
aspects of the model's input drive the decisions of a DNN. In countless
real-world domains, from legislation and law enforcement to healthcare, such
diagnosis is essential to ensure that DNN decisions are driven by aspects
appropriate in the context of its use. The development of methods and studies
enabling the explanation of a DNN's decisions has thus blossomed into an
active, broad area of research. A practitioner wanting to study explainable
deep learning may be intimidated by the plethora of orthogonal directions the
field has taken. This complexity is further exacerbated by competing
definitions of what it means ``to explain'' the actions of a DNN and to
evaluate an approach's ``ability to explain''. This article offers a field
guide to explore the space of explainable deep learning aimed at those
uninitiated in the field. The field guide: i) Introduces three simple
dimensions defining the space of foundational methods that contribute to
explainable deep learning, ii) discusses the evaluations for model
explanations, iii) places explainability in the context of other related deep
learning research areas, and iv) finally elaborates on user-oriented
explanation designing and potential future directions on explainable deep
learning. We hope the guide is used as an easy-to-digest starting point for
those just embarking on research in this field.},
   author = {Gabrielle Ras and Ning Xie and Marcel van Gerven and Derek Doran},
   doi = {10.1613/jair.1.13200},
   journal = {Journal of Artificial Intelligence Research},
   month = {4},
   pages = {329-397},
   publisher = {AI Access Foundation},
   title = {Explainable Deep Learning: A Field Guide for the Uninitiated},
   volume = {73},
   url = {https://arxiv.org/abs/2004.14545v2},
   year = {2020},
}
@article{Sokol2018,
   abstract = {Machine learning models have become pervasive in our everyday life; they decide on important matters influencing our education, employment and judicial system. Many of these predictive systems are commercial products protected by trade secrets, hence their decision-making is opaque. Therefore, in our research we address interpretability and explainability of predictions made by machine learning models. Our work draws heavily on human explanation research in social sciences: contrastive and exemplar explanations provided through a dialogue. This user-centric design, focusing on a lay audience rather than domain experts, applied to machine learning allows explainees to drive the explanation to suit their needs instead of being served a precooked template.},
   author = {Kacper Sokol and Peter Flach},
   doi = {10.24963/IJCAI.2018/836},
   isbn = {9780999241127},
   issn = {10450823},
   journal = {IJCAI International Joint Conference on Artificial Intelligence},
   pages = {5785-5786},
   publisher = {International Joint Conferences on Artificial Intelligence},
   title = {Conversational Explanations of Machine Learning Predictions Through Class-contrastive Counterfactual Statements},
   volume = {2018-July},
   year = {2018},
}
@article{Adadi2018,
   abstract = {At the dawn of the fourth industrial revolution, we are witnessing a fast and widespread adoption of artificial intelligence (AI) in our daily life, which contributes to accelerating the shift towards a more algorithmic society. However, even with such unprecedented advancements, a key impediment to the use of AI-based systems is that they often lack transparency. Indeed, the black-box nature of these systems allows powerful predictions, but it cannot be directly explained. This issue has triggered a new debate on explainable AI (XAI). A research field holds substantial promise for improving trust and transparency of AI-based systems. It is recognized as the sine qua non for AI to continue making steady progress without disruption. This survey provides an entry point for interested researchers and practitioners to learn key aspects of the young and rapidly growing body of research related to XAI. Through the lens of the literature, we review the existing approaches regarding the topic, discuss trends surrounding its sphere, and present major research trajectories.},
   author = {Amina Adadi and Mohammed Berrada},
   doi = {10.1109/ACCESS.2018.2870052},
   issn = {21693536},
   journal = {IEEE Access},
   keywords = {Explainable artificial intelligence,black-box models,interpretable machine learning},
   month = {9},
   pages = {52138-52160},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {Peeking Inside the Black-Box: A Survey on Explainable Artificial Intelligence (XAI)},
   volume = {6},
   year = {2018},
}
@article{DeGrave2021,
   abstract = {Artificial intelligence (AI) researchers and radiologists have recently reported AI systems that accurately detect COVID-19 in chest radiographs. However, the robustness of these systems remains unclear. Using state-of-the-art techniques in explainable AI, we demonstrate that recent deep learning systems to detect COVID-19 from chest radiographs rely on confounding factors rather than medical pathology, creating an alarming situation in which the systems appear accurate, but fail when tested in new hospitals. We observe that the approach to obtain training data for these AI systems introduces a nearly ideal scenario for AI to learn these spurious ‘shortcuts’. Because this approach to data collection has also been used to obtain training data for the detection of COVID-19 in computed tomography scans and for medical imaging tasks related to other diseases, our study reveals a far-reaching problem in medical-imaging AI. In addition, we show that evaluation of a model on external data is insufficient to ensure AI systems rely on medically relevant pathology, because the undesired ‘shortcuts’ learned by AI systems may not impair performance in new hospitals. These findings demonstrate that explainable AI should be seen as a prerequisite to clinical deployment of machine-learning healthcare models. The urgency of the developing COVID-19 epidemic has led to a large number of novel diagnostic approaches, many of which use machine learning. DeGrave and colleagues use explainable AI techniques to analyse a selection of these approaches and find that the methods frequently learn to identify features unrelated to the actual disease.},
   author = {Alex J. DeGrave and Joseph D. Janizek and Su In Lee},
   doi = {10.1038/s42256-021-00338-7},
   issn = {2522-5839},
   issue = {7},
   journal = {Nature Machine Intelligence 2021 3:7},
   keywords = {2,CoV,Computational science,Predictive medicine,Radiography,SARS},
   month = {5},
   pages = {610-619},
   pmid = {32995822},
   publisher = {Nature Publishing Group},
   title = {AI for radiographic COVID-19 detection selects shortcuts over signal},
   volume = {3},
   url = {https://www.nature.com/articles/s42256-021-00338-7},
   year = {2021},
}
@article{Doshi-Velez2017,
   abstract = {As machine learning systems become ubiquitous, there has been a surge of interest in interpretable machine learning: systems that provide explanation for their outputs. These explanations are often used to qualitatively assess other criteria such as safety or non-discrimination. However, despite the interest in interpretability, there is very little consensus on what interpretable machine learning is and how it should be measured. In this position paper, we first define interpretability and describe when interpretability is needed (and when it is not). Next, we suggest a taxonomy for rigorous evaluation and expose open questions towards a more rigorous science of interpretable machine learning.},
   author = {Finale Doshi-Velez and Been Kim},
   month = {2},
   title = {Towards A Rigorous Science of Interpretable Machine Learning},
   url = {http://arxiv.org/abs/1702.08608},
   year = {2017},
}
@article{Ribeiro2016,
   abstract = {Despite widespread adoption, machine learning models remain mostly black boxes. Understanding the reasons behind predictions is, however, quite important in assessing trust, which is fundamental if one plans to take action based on a prediction, or when choosing whether to deploy a new model. Such understanding also provides insights into the model, which can be used to transform an untrustworthy model or prediction into a trustworthy one. In this work, we propose LIME, a novel explanation technique that explains the predictions of any classifier in an interpretable and faithful manner, by learning an interpretable model locally varound the prediction. We also propose a method to explain models by presenting representative individual predictions and their explanations in a non-redundant way, framing the task as a submodular optimization problem. We demonstrate the flexibility of these methods by explaining different models for text (e.g. random forests) and image classification (e.g. neural networks). We show the utility of explanations via novel experiments, both simulated and with human subjects, on various scenarios that require trust: deciding if one should trust a prediction, choosing between models, improving an untrustworthy classifier, and identifying why a classifier should not be trusted.},
   author = {Marco Tulio Ribeiro and Sameer Singh and Carlos Guestrin},
   doi = {10.1145/2939672.2939778},
   journal = {Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
   title = {"Why Should I Trust You?": Explaining the Predictions of Any Classifier},
   url = {https://www.semanticscholar.org/paper/5091316bb1c6db6c6a813f4391911a5c311fdfe0},
   year = {2016},
}
@article{Linardatos2021,
   abstract = {Recent advances in artificial intelligence (AI) have led to its widespread industrial adoption, with machine learning systems demonstrating superhuman performance in a significant number of tasks. However, this surge in performance, has often been achieved through increased model complexity, turning such systems into “black box” approaches and causing uncertainty regarding the way they operate and, ultimately, the way that they come to decisions. This ambiguity has made it problematic for machine learning systems to be adopted in sensitive yet critical domains, where their value could be immense, such as healthcare. As a result, scientific interest in the field of Explainable Artificial Intelligence (XAI), a field that is concerned with the development of new methods that explain and interpret machine learning models, has been tremendously reignited over recent years. This study focuses on machine learning interpretability methods; more specifically, a literature review and taxonomy of these methods are presented, as well as links to their programming implementations, in the hope that this survey would serve as a reference point for both theorists and practitioners.},
   author = {Pantelis Linardatos and Vasilis Papastefanopoulos and Sotiris Kotsiantis},
   doi = {10.3390/E23010018},
   issn = {10994300},
   issue = {1},
   journal = {Entropy},
   keywords = {Black-box,Explainability,Fairness,Interpretability,Machine learning,Sensitivity,Xai},
   month = {1},
   pages = {1-45},
   pmid = {33375658},
   publisher = {Multidisciplinary Digital Publishing Institute  (MDPI)},
   title = {Explainable AI: A Review of Machine Learning Interpretability Methods},
   volume = {23},
   url = {/pmc/articles/PMC7824368/ /pmc/articles/PMC7824368/?report=abstract https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7824368/},
   year = {2021},
}
@article{Guidotti2018,
   abstract = {The recent years have witnessed the rise of accurate but obscure decision systems which hide the logic of their internal decision processes to the users. The lack of explanations for the decisions of black box systems is a key ethical issue, and a limitation to the adoption of machine learning components in socially sensitive and safety-critical contexts. Therefore, we need explanations that reveals the reasons why a predictor takes a certain decision. In this paper we focus on the problem of black box outcome explanation, i.e., explaining the reasons of the decision taken on a specific instance. We propose LORE, an agnostic method able to provide interpretable and faithful explanations. LORE first leans a local interpretable predictor on a synthetic neighborhood generated by a genetic algorithm. Then it derives from the logic of the local interpretable predictor a meaningful explanation consisting of: a decision rule, which explains the reasons of the decision; and a set of counterfactual rules, suggesting the changes in the instance's features that lead to a different outcome. Wide experiments show that LORE outperforms existing methods and baselines both in the quality of explanations and in the accuracy in mimicking the black box.},
   author = {Riccardo Guidotti and Anna Monreale and Salvatore Ruggieri and Dino Pedreschi and Franco Turini and Fosca Giannotti},
   month = {5},
   title = {Local Rule-Based Explanations of Black Box Decision Systems},
   url = {http://arxiv.org/abs/1805.10820},
   year = {2018},
}
@article{Mohseni2021,
   abstract = {The need for interpretable and accountable intelligent systems grows along with the prevalence of artificial intelligence (AI) applications used in everyday life. Explainable AI (XAI) systems are intended to self-explain the reasoning behind system decisions and predictions. Researchers from different disciplines work together to define, design, and evaluate explainable systems. However, scholars from different disciplines focus on different objectives and fairly independent topics of XAI research, which poses challenges for identifying appropriate design and evaluation methodology and consolidating knowledge across efforts. To this end, this article presents a survey and framework intended to share knowledge and experiences of XAI design and evaluation methods across multiple disciplines. Aiming to support diverse design goals and evaluation methods in XAI research, after a thorough review of XAI related papers in the fields of machine learning, vi-sualization, and human-computer interaction, we present a categorization of XAI design goals and evaluation methods. Our categorization presents the mapping between design goals for different XAI user groups and their evaluation methods. From our findings, we develop a framework with step-by-step design guidelines paired with evaluation methods to close the iterative design and evaluation cycles in multidisciplinary XAI teams. Further, we provide summarized ready-to-use tables of evaluation methods and recommendations for different goals in XAI research.},
   author = {Sina Mohseni and Niloofar Zarei and Eric D Ragan and ; E D Ragan},
   doi = {10.1145/3387166},
   isbn = {10.1145/3387166},
   issue = {4},
   journal = {ACM Transactions on Interactive Intelligent Systems},
   keywords = {CCS Concepts:,Computing methodologies → Machine learning Additio,Human-centered computing → HCI design and evaluati},
   title = {24 A Multidisciplinary Survey and Framework for Design and Evaluation of Explainable AI Systems},
   volume = {11},
   url = {https://doi.org/10.1145/3387166},
   year = {2021},
}
@article{Keane2019,
   abstract = {This paper proposes a theoretical analysis of one approach to the eXplainable AI (XAI) problem, using post-hoc explanation-by-example, that relies on the twinning of artificial neural networks (ANNs) with case-based reasoning (CBR) systems; so-called ANN-CBR twins. It surveys these systems to advance a new theoretical interpretation of previous work and define a road map for CBR’s further role in XAI. A systematic survey of 1,102 papers was conducted to identify a fragmented literature on this topic and trace its influence to more recent work involving deep neural networks (DNNs). The twin-systems approach is advanced as one possible coherent, generic solution to the XAI problem. The paper concludes by road-mapping future directions for this XAI solution, considering (i) further tests of feature-weighting techniques, (ii) how explanatory cases might be deployed (e.g., in counterfactuals, a fortori cases), and (iii) the unwelcome, much-ignored issue of user evaluation.},
   author = {Mark T. Keane and Eoin M. Kenny},
   doi = {10.1007/978-3-030-29249-2_11},
   isbn = {9783030292485},
   issn = {16113349},
   journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   keywords = {Artificial neural networks,CBR,Deep learning,Explanation,XAI},
   pages = {155-171},
   publisher = {Springer Verlag},
   title = {How Case-Based Reasoning Explains Neural Networks: A Theoretical Analysis of XAI Using Post-Hoc Explanation-by-Example from a Survey of ANN-CBR Twin-Systems},
   volume = {11680 LNAI},
   year = {2019},
}
@inproceedings{Keane2020,
   abstract = {Recently, a groundswell of research has identified the use of counterfactual explanations as a potentially significant solution to the Explainable AI (XAI) problem. It is argued that (i) technically, these counterfactual cases can be generated by permuting problem-features until a class-change is found, (ii) psychologically, they are much more causally informative than factual explanations, (iii) legally, they are GDPR-compliant. However, there are issues around the finding of “good” counterfactuals using current techniques (e.g. sparsity and plausibility). We show that many commonly-used datasets appear to have few “good” counterfactuals for explanation purposes. So, we propose a new case-based approach for generating counterfactuals, using novel ideas about the counterfactual potential and explanatory coverage of a case-base. The new technique reuses patterns of good counterfactuals, present in a case-base, to generate analogous counterfactuals that can explain new problems and their solutions. Several experiments show how this technique can improve the counterfactual potential and explanatory coverage of case-bases that were previously found wanting.},
   author = {Mark T. Keane and Barry Smyth},
   doi = {10.1007/978-3-030-58342-2_11},
   isbn = {9783030583415},
   issn = {16113349},
   journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   keywords = {CBR,Contrastive,Counterfactuals,Explanation,XAI},
   pages = {163-178},
   publisher = {Springer Science and Business Media Deutschland GmbH},
   title = {Good Counterfactuals and Where to Find Them: A Case-Based Technique for Generating Counterfactuals for Explainable AI (XAI)},
   volume = {12311 LNAI},
   year = {2020},
}
@article{Caruana2015,
   abstract = {In machine learning often a tradeoff must be made between accuracy and intelligibility. More accurate models such as boosted trees, random forests, and neural nets usually are not intelligible, but more intelligible models such as logistic regression, naive-Bayes, and single decision trees often have significantly worse accuracy. This tradeoff sometimes limits the accuracy of models that can be applied in mission-critical applications such as healthcare where being able to understand, validate, edit, and trust a learned model is important. We present two case studies where high-performance generalized additive models with pairwise interactions (GA2Ms) are applied to real healthcare problems yielding intelligible models with state-of-the-art accuracy. In the pneumonia risk prediction case study, the intelligible model uncovers surprising patterns in the data that previously had prevented complex learned models from being fielded in this domain, but because it is intelligible and modular allows these patterns to be recognized and removed. In the 30-day hospital readmission case study, we show that the same methods scale to large datasets containing hundreds of thousands of patients and thousands of attributes while remaining intelligible and providing accuracy comparable to the best (unintelligible) machine learning methods.},
   author = {Rich Caruana and Yin Lou and Johannes Gehrke and Paul Koch and Marc Sturm and Noémie Elhadad},
   doi = {10.1145/2783258.2788613},
   isbn = {9781450336642},
   journal = {Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
   keywords = {Additive models,Classification,Healthcare,Intelligibility,Interaction detection,Logistic regression,Risk prediction},
   month = {8},
   note = {This isn't so much a solution - or patient facing - but just a more interpretable model<br/>This looks very data-focussed, but interesting for the technicians/AI experts},
   pages = {1721-1730},
   publisher = {Association for Computing Machinery},
   title = {Intelligible models for healthcare: Predicting pneumonia risk and hospital 30-day readmission},
   volume = {2015-Augus},
   year = {2015},
}
@article{Lipton2018,
   author = {Zachary C. Lipton},
   doi = {10.1145/3233231},
   issn = {15577317},
   issue = {10},
   journal = {Communications of the ACM},
   month = {10},
   note = {Discorse of interpretability},
   pages = {35-43},
   publisher = {Association for Computing Machinery},
   title = {The mythos of model interpretability},
   volume = {61},
   url = {https://arxiv.org/abs/1606.03490},
   year = {2018},
}
@article{Goldstein2021,
   abstract = {With machine learning models being increasingly used to aid decision making even in high-stakes domains, there has been a growing interest in developing interpretable models. Although many supposedly interpretable models have been proposed, there have been relatively few experimental studies investigating whether these models achieve their intended effects, such as making people more closely follow a model's predictions when it is beneficial for them to do so or enabling them to detect when a model has made a mistake. We present a sequence of pre-registered experiments (í µí± = 3, 800) in which we showed participants functionally identical models that varied only in two factors commonly thought to make machine learning models more or less interpretable: the number of features and the transparency of the model (i.e., whether the model internals are clear or black box). Predictably, participants who saw a clear model with few features could better simulate the model's predictions. However, we did not find that participants more closely followed its predictions. Furthermore, showing participants a clear model meant that they were less able to detect and correct for the model's sizable mistakes, seemingly due to information overload. These counterintuitive findings emphasize the importance of testing over intuition when developing interpretable models.},
   author = {Daniel G Goldstein and Jake M Hofman and Microsoft Research JENNIFER WORTMAN VAUGHAN and Forough Poursabzi-Sangdeh and Jennifer Wortman Vaughan and Hanna Wallach},
   doi = {10.1145/3411764.3445315},
   isbn = {9781450380966},
   keywords = {CCS Concepts:,Computing methodologies → Machine learning;,Human-centered computing → User studies Additional},
   note = {We could use this information to flesh out the background we give to participants. Probably realistic that they might be told the upsides/downsides of different model interpretabilities, and particularly relevant that its harder to correct mistakes for glass-box models.<br/><br/>Very on-topic, focusses on whether a patient can mirror a model. Interesting counterpoint for us: does the patient need to understand the model? They probably don't understand the medical diagnosis process normally so why is it expected that with AI tools they know the inner workings?},
   publisher = {ACM},
   title = {Manipulating and Measuring Model Interpretability},
   volume = {67},
   url = {https://doi.org/10.1145/3411764.3445315},
   year = {2021},
}
@article{Guefrechi2021,
   abstract = {The whole world is facing a health crisis, that is unique in its kind, due to the COVID-19 pandemic. As the coronavirus continues spreading, researchers are concerned by providing or help provide solutions to save lives and to stop the pandemic outbreak. Among others, artificial intelligence (AI) has been adapted to address the challenges caused by pandemic. In this article, we design a deep learning system to extract features and detect COVID-19 from chest X-ray images. Three powerful networks, namely ResNet50, InceptionV3, and VGG16, have been fine-tuned on an enhanced dataset, which was constructed by collecting COVID-19 and normal chest X-ray images from different public databases. We applied data augmentation techniques to artificially generate a large number of chest X-ray images: Random Rotation with an angle between − 10 and 10 degrees, random noise, and horizontal flips. Experimental results are encouraging: the proposed models reached an accuracy of 97.20 \% for Resnet50, 98.10 \% for InceptionV3, and 98.30 \% for VGG16 in classifying chest X-ray images as Normal or COVID-19. The results show that transfer learning is proven to be effective, showing strong performance and easy-to-deploy COVID-19 detection methods. This enables automatizing the process of analyzing X-ray images with high accuracy and it can also be used in cases where the materials and RT-PCR tests are limited.},
   author = {Sarra Guefrechi and Marwa Ben Jabra and Adel Ammar and Anis Koubaa and Habib Hamam},
   doi = {10.1007/S11042-021-11192-5},
   issn = {15737721},
   issue = {21-23},
   journal = {Multimedia Tools and Applications},
   keywords = {CNN,COVID-19,Chest X-ray,Convolution Neural Network,Deep learning},
   month = {9},
   pages = {31803},
   pmid = {34305440},
   publisher = {Nature Publishing Group},
   title = {Deep learning based detection of COVID-19 from chest X-ray images},
   volume = {80},
   url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8286881/},
   year = {2021},
}
@article{Holstein2019,
   abstract = {The potential for machine learning (ML) systems to amplify social inequities and unfairness is receiving increasing popular and academic attention. A surge of recent work has focused on the development of algorithmic tools to assess and mitigate such unfairness. If these tools are to have a positive impact on industry practice, however, it is crucial that their design be informed by an understanding of real-world needs. Through 35 semi-structured interviews and an anonymous survey of 267 ML practitioners, we conduct the first systematic investigation of commercial product teams’ challenges and needs for support in developing fairer ML systems. We identify areas of alignment and disconnect between the challenges faced by teams in practice and the solutions proposed in the fair ML research literature. Based on these findings, we highlight directions for future ML and HCI research that will better address practitioners’ needs.},
   author = {Kenneth Holstein and Jennifer Wortman Vaughan and Hal Daumé and Miroslav Dudík and Hanna Wallach},
   doi = {10.1145/3290605.3300830},
   isbn = {9781450359702},
   journal = {Conference on Human Factors in Computing Systems - Proceedings},
   keywords = {Algorithmic bias,Empirical study,Fair machine learning,Need-finding,Product teams,UX of machine learning},
   month = {5},
   publisher = {Association for Computing Machinery},
   title = {Improving fairness in machine learning systems: What do industry practitioners need?},
   year = {2019},
}
%not sure how this one is related?
@article{Kim2021,
   abstract = {A stimulus-response system and conscious response enable humans to respond effectively to environmental changes and external stimuli. This paper presents an artificial stimulus-response system that is inspired by human conscious response and is capable of emulating it. The system is composed of an artificial visual receptor, artificial synapse, artificial neuron circuits, and actuator. By incorporating these artificial nervous components, a series of conscious response processes that markedly reduces response time as a result of learning from repeated stimuli are demonstrated. The proposed artificial stimulus-response system offers the promise of a new research field that would aid the development of artificial intelligence-based organs for patients with neurological disorders.},
   author = {Seongchan Kim and Dong Gue Roe and Yoon Young Choi and Hwije Woo and Joongpill Park and Jong Ik Lee and Yongsuk Choi and Sae Byeok Jo and Moon Sung Kang and Young Jae Song and Sohee Jeong and Jeong Ho Cho},
   doi = {10.1126/SCIADV.ABE3996},
   issn = {23752548},
   issue = {15},
   journal = {Science Advances},
   month = {4},
   pmid = {33837079},
   publisher = {American Association for the Advancement of Science},
   title = {Artificial stimulus-response system capable of conscious response},
   volume = {7},
   year = {2021},
}
@article{Koh2017,
   abstract = {How can we explain the predictions of a black-box model? In this paper, we use influence functions - a classic technique from robust statistics - to trace a model's prediction through the learning algorithm and back to its training data, thereby identifying training points most responsible for a given prediction. To scale up influence functions to modern machine learning settings, we develop a simple, efficient implementation that requires only oracle access to gradients and Hessian-vector products. We show that even on non-convex and non-differentiable models where the theory breaks down, approximations to influence functions can still provide valuable information. On linear models and convolutional neural networks, we demonstrate that influence functions are useful for multiple purposes: understanding model behavior, debugging models, detecting dataset errors, and even creating visually-indistinguishable training-set attacks.},
   author = {Pang Wei Koh and Percy Liang},
   isbn = {9781510855144},
   journal = {34th International Conference on Machine Learning, ICML 2017},
   pages = {2976-2987},
   publisher = {International Machine Learning Society (IMLS)},
   title = {Understanding black-box predictions via influence functions},
   volume = {4},
   year = {2017},
}
@article{Gunning2019,
   abstract = {Recent successes in machine learning (ML) have led to a new wave of artificial intelligence (AI) applications that offer extensive benefits to a diverse range of fields. However, many of these systems are not able to explain their autonomous decisions and actions to human users. Explanations may not be essential for certain AI applications, and some AI researchers argue that the emphasis on explanation is misplaced, too difficult to achieve, and perhaps unnecessary. However, for many critical applications in defense, medicine, finance, and law, explanations are essential for users to understand, trust, and effectively manage these new, artificially intelligent partners [see recent reviews (1-3)].},
   author = {David Gunning and Mark Stefik and Jaesik Choi and Timothy Miller and Simone Stumpf and Guang Zhong Yang},
   doi = {10.1126/SCIROBOTICS.AAY7120/ASSET/635AF7F2-CA10-40DD-87D2-7DCD424AE2CD/ASSETS/GRAPHIC/AAY7120-F1.JPEG},
   issn = {24709476},
   issue = {37},
   journal = {Science Robotics},
   month = {12},
   note = {Some relevant stuff, even discuss the idea that explainability might not be necessary for some users in the pipeline.},
   pmid = {33137719},
   publisher = {American Association for the Advancement of Science},
   title = {XAI-Explainable artificial intelligence},
   volume = {4},
   url = {https://www.science.org/doi/abs/10.1126/scirobotics.aay7120},
   year = {2019},
}
@article{Vilone2021,
   abstract = {Explainable Artificial Intelligence (XAI) has experienced a significant growth over the last few years. This is due to the widespread application of machine learning, particularly deep learning, that has led to the development of highly accurate models that lack explainability and interpretability. A plethora of methods to tackle this problem have been proposed, developed and tested, coupled with several studies attempting to define the concept of explainability and its evaluation. This systematic review contributes to the body of knowledge by clustering all the scientific studies via a hierarchical system that classifies theories and notions related to the concept of explainability and the evaluation approaches for XAI methods. The structure of this hierarchy builds on top of an exhaustive analysis of existing taxonomies and peer-reviewed scientific material. Findings suggest that scholars have identified numerous notions and requirements that an explanation should meet in order to be easily understandable by end-users and to provide actionable information that can inform decision making. They have also suggested various approaches to assess to what degree machine-generated explanations meet these demands. Overall, these approaches can be clustered into human-centred evaluations and evaluations with more objective metrics. However, despite the vast body of knowledge developed around the concept of explainability, there is not a general consensus among scholars on how an explanation should be defined, and how its validity and reliability assessed. Eventually, this review concludes by critically discussing these gaps and limitations, and it defines future research directions with explainability as the starting component of any artificial intelligent system.},
   author = {Giulia Vilone and Luca Longo},
   doi = {10.1016/J.INFFUS.2021.05.009},
   issn = {1566-2535},
   journal = {Information Fusion},
   keywords = {Evaluation methods,Explainable artificial intelligence,Notions of explainability},
   month = {12},
   pages = {89-106},
   publisher = {Elsevier},
   title = {Notions of explainability and evaluation approaches for explainable artificial intelligence},
   volume = {76},
   year = {2021},
}
@article{Wang2019,
   abstract = {From healthcare to criminal justice, artificial intelligence (AI) is increasingly supporting high-consequence human decisions. This has spurred the field of explainable AI (XAI). This paper seeks to strengthen empirical application-specific investigations of XAI by exploring theoretical underpinnings of human decision making, drawing from the fields of philosophy and psychology. In this paper, we propose a conceptual framework for building human-centered, decision-theory-driven XAI based on an extensive review across these fields. Drawing on this framework, we identify pathways along which human cognitive patterns drives needs for building XAI and how XAI can mitigate common cognitive biases. We then put this framework into practice by designing and implementing an explainable clinical diagnostic tool for intensive care phenotyping and conducting a co-design exercise with clinicians. Thereafter, we draw insights into how this framework bridges algorithm-generated explanations and human decision-making theories. Finally, we discuss implications for XAI design and development. CCS CONCEPTS • Human-centered computing ~ Human computer interaction From supporting healthcare intervention decisions to informing criminal justice, artificial intelligence (AI) is now increasingly entering the mainstream and supporting high-consequence human decisions. However, the effectiveness of these systems will be limited by the machine's inability to explain its thoughts and actions to human users in these critical situations. These challenges have spurred research interest in explainable AI (XAI) [2, 12, 32, 43, 109]. To enable end users to understand, trust, and effectively manage their intelligent partners, HCI and AI researchers have produced many user-centered, innovative algorithm visualizations, interfaces and toolkits (e.g., [18, 56, 67, 86] that support users with various levels of AI literacy in diverse subject domains, from the bank customer who is refused a loan, the doctor making a diagnosis with a decision aid, to the patient who learns that he may have skin cancer from a smartphone photograph of his mole [30]. Adding on to this line of inquiry, this paper seeks to strengthen empirical application-specific investigations of XAI by exploring theoretical underpinnings of human decision making, drawing from the fields of philosophy and psychology. We first conducted an extensive literature review in cognitive psychology, philosophy and decision-making theories that describe patterns of how people reason, make decisions and seek explanations, and cognitive factors that bias or compromise decision-making.},
   author = {Danding Wang and Qian Yang and Ashraf Abdul and Brian Y Lim},
   city = {New York, NY, USA},
   doi = {10.1145/3290605},
   isbn = {9781450359702},
   journal = {Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems},
   keywords = {Clinical decision making,Decision making,Explainable artificial intelligence,Explanations,Intelligibility},
   note = {This has got the wrong paper attached, I've re-added the right one},
   publisher = {ACM},
   title = {Designing Theory-Driven User-Centric Explainable AI},
   url = {https://doi.org/10.1145/3290605.3300831},
   year = {2019},
}
@article{Poyiadzi2020,
   abstract = {Work in Counterfactual Explanations tends to focus on the principle of "the closest possible world" that identifies small changes leading to the desired outcome. In this paper we argue that while this approach might initially seem intuitively appealing it exhibits shortcomings not addressed in the current literature. First, a counterfactual example generated by the state-of-the-art systems is not necessarily representative of the underlying data distribution, and may therefore prescribe unachievable goals (e.g., an unsuccessful life insurance applicant with severe disability may be advised to do more sports). Secondly, the counterfactuals may not be based on a "feasible path" between the current state of the subject and the suggested one, making actionable recourse infeasible (e.g., lowskilled unsuccessful mortgage applicants may be told to double their salary, which may be hard without first increasing their skill level). These two shortcomings may render counterfactual explanations impractical and sometimes outright offensive. To address these two major flaws, first of all, we propose a new line of Counterfactual Explanations research aimed at providing actionable and feasible paths to transform a selected instance into one that meets a certain goal. Secondly, we propose FACE: an algorithmically sound way of uncovering these "feasible paths" based on the shortest path distances defined via density-weighted metrics. Our approach generates counterfactuals that are coherent with the underlying data distribution and supported by the "feasible paths" of change, which are achievable and can be tailored to the problem at hand.},
   author = {Rafael Poyiadzi and Kacper Sokol and Raul Santos-Rodriguez and Tijl De Bie and Peter Flach},
   doi = {10.1145/3375627.3375850},
   isbn = {9781450371100},
   journal = {AIES 2020 - Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society},
   keywords = {Black-box Models,Counterfactuals,Explainability,Interpretability},
   month = {2},
   pages = {344-350},
   publisher = {Association for Computing Machinery, Inc},
   title = {FACE: Feasible and actionable counterfactual explanations},
   year = {2020},
}
@article{Holzinger2017,
   abstract = {Artificial intelligence (AI) generally and machine learning (ML) specifically demonstrate impressive practical success in many different application domains, e.g. in autonomous driving, speech recognition, or recommender systems. Deep learning approaches, trained on extremely large data sets or using reinforcement learning methods have even exceeded human performance in visual tasks, particularly on playing games such as Atari, or mastering the game of Go. Even in the medical domain there are remarkable results. The central problem of such models is that they are regarded as black-box models and even if we understand the underlying mathematical principles, they lack an explicit declarative knowledge representation, hence have difficulty in generating the underlying explanatory structures. This calls for systems enabling to make decisions transparent, understandable and explainable. A huge motivation for our approach are rising legal and privacy aspects. The new European General Data Protection Regulation entering into force on May 25th 2018, will make black-box approaches difficult to use in business. This does not imply a ban on automatic learning approaches or an obligation to explain everything all the time, however, there must be a possibility to make the results re-traceable on demand. In this paper we outline some of our research topics in the context of the relatively new area of explainable-AI with a focus on the application in medicine, which is a very special domain. This is due to the fact that medical professionals are working mostly with distributed heterogeneous and complex sources of data. In this paper we concentrate on three sources: images, *omics data and text. We argue that research in explainable-AI would generally help to facilitate the implementation of AI/ML in the medical domain, and specifically help to facilitate transparency and trust.},
   author = {Andreas Holzinger and Chris Biemann and Constantinos S. Pattichis and Douglas B. Kell},
   issn = {2331-8422},
   month = {12},
   title = {What do we need to build explainable AI systems for the medical domain?},
   url = {http://arxiv.org/abs/1712.09923},
   year = {2017},
}
@article{Abdul2018,
   abstract = {Advances in artificial intelligence, sensors and big data man-agement have far-reaching societal impacts. As these sys-tems augment our everyday lives, it becomes increasingly important for people to understand them and remain in con-trol. We investigate how HCI researchers can help to develop accountable systems by performing a literature analysis of 289 core papers on explanations and explainable systems, as well as 12,412 citing papers. Using topic modeling, co-oc-currence and network analysis, we mapped the research space from diverse domains, such as algorithmic accounta-bility, interpretable machine learning, context-awareness, cognitive psychology, and software learnability. We reveal fading and burgeoning trends in explainable systems, and identify domains that are closely connected or mostly iso-lated. The time is ripe for the HCI community to ensure that the powerful new autonomous systems have intelligible in-terfaces built-in. From our results, we propose several impli-cations and directions for future research towards this goal.},
   author = {Ashraf Abdul and Jo Vermeulen and Danding Wang and Brian Y. Lim and Mohan Kankanhalli},
   doi = {10.1145/3173574.3174156},
   isbn = {9781450356206},
   journal = {Conference on Human Factors in Computing Systems - Proceedings},
   keywords = {Explainable artificial intelli-gence,Explanations,Intelligibility,Interpretable machine learning},
   month = {4},
   publisher = {Association for Computing Machinery},
   title = {Trends and trajectories for explainable, accountable and intelligible systems: An HCI research agenda},
   volume = {2018-April},
   year = {2018},
}
@article{Miller2021,
   abstract = {This paper presents a model of contrastive explanation using structural casual models. The topic of causal explanation in artificial intelligence has gathered interest in recent years as researchers and practitioners aim to increase trust and understanding of intelligent decision-making. While different sub-fields of artificial intelligence have looked into this problem with a sub-field-specific view, there are few models that aim to capture explanation more generally. One general model is based on structural causal models. It defines an explanation as a fact that, if found to be true, would constitute an actual cause of a specific event. However, research in philosophy and social sciences shows that explanations are contrastive: that is, when people ask for an explanation of an event - the fact - they (sometimes implicitly) are asking for an explanation relative to some contrast case; that is, 'Why P rather than Q?'. In this paper, we extend the structural causal model approach to define two complementary notions of contrastive explanation, and demonstrate them on two classical problems in artificial intelligence: classification and planning. We believe that this model can help researchers in subfields of artificial intelligence to better understand contrastive explanation.},
   author = {Tim Miller},
   doi = {10.1017/S0269888921000102},
   issn = {14698005},
   issue = {1},
   journal = {Knowledge Engineering Review},
   month = {10},
   publisher = {Cambridge University Press},
   title = {Contrastive explanation: A structural-model approach},
   volume = {36},
   year = {2021},
}
@article{Guidotti2018a,
   abstract = {In recent years, many accurate decision support systems have been constructed as black boxes, that is as systems that hide their internal logic to the user. This lack of explanation constitutes bot...},
   author = {Riccardo Guidotti and Anna Monreale and Salvatore Ruggieri and Franco Turini and Fosca Giannotti and Dino Pedreschi},
   doi = {10.1145/3236009},
   issn = {15577341},
   issue = {5},
   journal = {ACM Computing Surveys (CSUR)},
   keywords = {Open the black box,explanations,interpretability,transparent models},
   month = {8},
   publisher = {
		ACM
		PUB27
		New York, NY, USA
	},
   title = {A Survey of Methods for Explaining Black Box Models},
   volume = {51},
   url = {https://dl.acm.org/doi/abs/10.1145/3236009},
   year = {2018},
}
@article{Narayanan2018,
   abstract = {Recent years have seen a boom in interest in machine learning systems that can provide a human-understandable rationale for their predictions or decisions. However, exactly what kinds of explanation are truly human-interpretable remains poorly understood. This work advances our understanding of what makes explanations interpretable in the specific context of verification. Suppose we have a machine learning system that predicts X, and we provide rationale for this prediction X. Given an input, an explanation, and an output, is the output consistent with the input and the supposed rationale? Via a series of user-studies, we identify what kinds of increases in complexity have the greatest effect on the time it takes for humans to verify the rationale, and which seem relatively insensitive.},
   author = {Menaka Narayanan and Emily Chen and Jeffrey He and Been Kim and Sam Gershman and Finale Doshi-Velez},
   issn = {2331-8422},
   month = {2},
   title = {How do Humans Understand Explanations from Machine Learning Systems? An Evaluation of the Human-Interpretability of Explanation},
   url = {http://arxiv.org/abs/1802.00682},
   year = {2018},
}
@article{Sokol2019,
   abstract = {Surrogate explainers of black-box machine learning predictions are of paramount importance in the field of eXplainable Artificial Intelligence since they can be applied to any type of data (images, text and tabular), are model-agnostic and are post-hoc (i.e., can be retrofitted). The Local Interpretable Model-agnostic Explanations (LIME) algorithm is often mistakenly unified with a more general framework of surrogate explainers, which may lead to a belief that it is the solution to surrogate explainability. In this paper we empower the community to "build LIME yourself" (bLIMEy) by proposing a principled algorithmic framework for building custom local surrogate explainers of black-box model predictions, including LIME itself. To this end, we demonstrate how to decompose the surrogate explainers family into algorithmically independent and interoperable modules and discuss the influence of these component choices on the functional capabilities of the resulting explainer, using the example of LIME.},
   author = {K Sokol and A Hepburn and R Santos-Rodriguez and P Flach},
   title = {bLIMEy: Surrogate Prediction Explanations Beyond LIME},
   year = {2019},
}
@article{Bohr2020,
    abstract = {Big data and machine learning are having an impact on most aspects of modern life, from entertainment, commerce, and healthcare. Netflix knows which films and series people prefer to watch, Amazon knows which items people like to buy when and where, and Google knows which symptoms and conditions people are searching for. All this data can be used for very detailed personal profiling, which may be of great value for behavioral understanding and targeting but also has potential for predicting healthcare trends. There is great optimism that the application of artificial intelligence (AI) can provide substantial improvements in all areas of healthcare from diagnostics to treatment. It is generally believed that AI tools will facilitate and enhance human work and not replace the work of physicians and other healthcare staff as such. AI is ready to support healthcare personnel with a variety of tasks from administrative workflow to clinical documentation and patient outreach as well as specialized support such as in image analysis, medical device automation, and patient monitoring. In this chapter, some of the major applications of AI in healthcare will be discussed covering both the applications that are directly associated with healthcare and those in the healthcare value chain such as drug development and ambient assisted living.},
    author = {Bohr, Adam and Memarzadeh, Kaveh},
    doi = {10.1016/B978-0-12-818438-7.00002-2},
    isbn = {9780128184387},
    journal = {Artificial Intelligence in Healthcare},
    keywords = {Ambient assisted living,Artificial intelligence,Healthcare applications,Machine learning,Machine vision,Natural language programming,Precision medicine},
    month = {jan},
    pages = {25},
    publisher = {Elsevier},
    title = {{The rise of artificial intelligence in healthcare applications}},
    year = {2020}
}
@article{Chen2018,
    abstract = {Over the past decade, deep learning has achieved remarkable success in various artificial intelligence research areas. Evolved from the previous research on artificial neural networks, this technology has shown superior performance to other machine learning algorithms in areas such as image and voice recognition, natural language processing, among others. The first wave of applications of deep learning in pharmaceutical research has emerged in recent years, and its utility has gone beyond bioactivity predictions and has shown promise in addressing diverse problems in drug discovery. Examples will be discussed covering bioactivity prediction, de novo molecular design, synthesis prediction and biological image analysis.},
    author = {Chen, Hongming and Engkvist, Ola and Wang, Yinhai and Olivecrona, Marcus and Blaschke, Thomas},
    doi = {10.1016/J.DRUDIS.2018.01.039},
    file = {::},
    issn = {1359-6446},
    journal = {Drug Discovery Today},
    mendeley-groups = {Medical Domain},
    month = {jun},
    number = {6},
    pages = {1241--1250},
    pmid = {29366762},
    publisher = {Elsevier Current Trends},
    title = {{The rise of deep learning in drug discovery}},
    volume = {23},
    year = {2018}
}
@article{Wachter2017,
   abstract = {Since approval of the EU General Data Protection Regulation (GDPR) in 2016, it has been widely and repeatedly claimed that the GDPR will legally mandate a ‘right to explanation’ of all decisions made by automated or artificially intelligent algorithmic systems. This right to explanation is viewed as an ideal mechanism to enhance the accountability and transparency of automated decision-making. However, there are several reasons to doubt both the legal existence and the feasibility of such a right. In contrast to the right to explanation of specific automated decisions claimed elsewhere, the GDPR only mandates that data subjects receive meaningful, but properly limited, information (Articles 13-15) about the logic involved, as well as the significance and the envisaged consequences of automated decision-making systems, what we term a ‘right to be informed’. Further, the ambiguity and limited scope of the ‘right not to be subject to automated decision-making’ contained in Article 22 (from which the alleged ‘right to explanation’ stems) raises questions over the protection actually afforded to data subjects. These problems show that the GDPR lacks precise language as well as explicit and well-defined rights and safeguards against automated decision-making, and therefore runs the risk of being toothless. We propose a number of legislative and policy steps that, if taken, may improve the transparency and accountability of automated decision-making when the GDPR comes into force in 2018.},
   author = {Sandra Wachter and Brent Mittelstadt and Luciano Floridi},
   doi = {10.2139/SSRN.2903469},
   journal = {SSRN Electronic Journal},
   keywords = {Brent Mittelstadt,General Data Protection Regulation,Luciano Floridi,SSRN,Sandra Wachter,algorithms,artificial intelligence,automated decision-making,data protection,right of access,right to explanation},
   month = {12},
   publisher = {Elsevier BV},
   title = {Why a Right to Explanation of Automated Decision-Making Does Not Exist in the General Data Protection Regulation},
   url = {https://papers.ssrn.com/abstract=2903469},
   year = {2017},
}
@article{Arrieta2020,
   abstract = {In the last few years, Artificial Intelligence (AI) has achieved a notable momentum that, if harnessed appropriately, may deliver the best of expectations over many application sectors across the field. For this to occur shortly in Machine Learning, the entire community stands in front of the barrier of explainability, an inherent problem of the latest techniques brought by sub-symbolism (e.g. ensembles or Deep Neural Networks) that were not present in the last hype of AI (namely, expert systems and rule based models). Paradigms underlying this problem fall within the so-called eXplainable AI (XAI) field, which is widely acknowledged as a crucial feature for the practical deployment of AI models. The overview presented in this article examines the existing literature and contributions already done in the field of XAI, including a prospect toward what is yet to be reached. For this purpose we summarize previous efforts made to define explainability in Machine Learning, establishing a novel definition of explainable Machine Learning that covers such prior conceptual propositions with a major focus on the audience for which the explainability is sought. Departing from this definition, we propose and discuss about a taxonomy of recent contributions related to the explainability of different Machine Learning models, including those aimed at explaining Deep Learning methods for which a second dedicated taxonomy is built and examined in detail. This critical literature analysis serves as the motivating background for a series of challenges faced by XAI, such as the interesting crossroads of data fusion and explainability. Our prospects lead toward the concept of Responsible Artificial Intelligence, namely, a methodology for the large-scale implementation of AI methods in real organizations with fairness, model explainability and accountability at its core. Our ultimate goal is to provide newcomers to the field of XAI with a thorough taxonomy that can serve as reference material in order to stimulate future research advances, but also to encourage experts and professionals from other disciplines to embrace the benefits of AI in their activity sectors, without any prior bias for its lack of interpretability.},
   author = {Alejandro Barredo Arrieta and Natalia Díaz-Rodríguez and Javier Del Ser and Adrien Bennetot and Siham Tabik and Alberto Barbado and Salvador Garcia and Sergio Gil-Lopez and Daniel Molina and Richard Benjamins and Raja Chatila and Francisco Herrera},
   doi = {10.1016/J.INFFUS.2019.12.012},
   issn = {1566-2535},
   journal = {Information Fusion},
   keywords = {Accountability,Comprehensibility,Data Fusion,Deep Learning,Explainable Artificial Intelligence,Fairness,Interpretability,Machine Learning,Privacy,Responsible Artificial Intelligence,Transparency},
   month = {6},
   pages = {82-115},
   publisher = {Elsevier},
   title = {Explainable Artificial Intelligence (XAI): Concepts, taxonomies, opportunities and challenges toward responsible AI},
   volume = {58},
   year = {2020},
}
@article{Verma2020,
   abstract = {Machine learning plays a role in many deployed decision systems, often in
ways that are difficult or impossible to understand by human stakeholders.
Explaining, in a human-understandable way, the relationship between the input
and output of machine learning models is essential to the development of
trustworthy machine-learning-based systems. A burgeoning body of research seeks
to define the goals and methods of explainability in machine learning. In this
paper, we seek to review and categorize research on counterfactual
explanations, a specific class of explanation that provides a link between what
could have happened had input to a model been changed in a particular way.
Modern approaches to counterfactual explainability in machine learning draw
connections to the established legal doctrine in many countries, making them
appealing to fielded systems in high-impact areas such as finance and
healthcare. Thus, we design a rubric with desirable properties of
counterfactual explanation algorithms and comprehensively evaluate all
currently-proposed algorithms against that rubric. Our rubric provides easy
comparison and comprehension of the advantages and disadvantages of different
approaches and serves as an introduction to major research themes in this
field. We also identify gaps and discuss promising research directions in the
space of counterfactual explainability.},
   author = {Sahil Verma and Arthur Ai and John Dickerson and Keegan Hines},
   doi = {10.48550/arxiv.2010.10596},
   month = {10},
   title = {Counterfactual Explanations for Machine Learning: A Review},
   url = {https://arxiv.org/abs/2010.10596v1},
   year = {2020},
}
@article{Salimiparsa2021,
   abstract = {Clinical decision support (CDS) systems are computer applications whose goal is to facilitate the decision-making process of clinicians. In recent years, CDSS has developed an interest in applying machine learning (ML) models to make predictions related to clinical outcomes. The limited interpretability of many ML models is a major barrier to clinical adoption. This challenge has sparked research interest in interpretable and explainable AI, commonly known as XAI. XAI methods are used to construct and communicate explanations of the predictions made by machine learning models so that end users can interpret those predictions. However, these methods are not designed based on end-users' needs; rather, they are based on the developers’ intuitions of what a good explanation is. Furthermore, XAI methods are not tailored to the specific tasks that a user will undertake, nor are they tailored to the interface used to perform those tasks. To tackle these issues, we propose to develop a visual analytic tool to explain an ML model for clinical applications whose design will explicitly take into account the context of tasks and the needs of end-users.},
   author = {Mozhgan Salimiparsa and Daniel Lizotte and Kamran Sedig},
   doi = {10.21428/594757DB.62860442},
   journal = {Proceedings of the Canadian Conference on Artificial Intelligence},
   month = {6},
   publisher = {PubPub},
   title = {A User-Centered Design of Explainable AI for Clinical Decision Support},
   url = {https://caiac.pubpub.org/pub/es06632p/release/1},
   year = {2021},
}
@article{Liao2020,
   abstract = {A surge of interest in explainable AI (XAI) has led to a vast collection of algorithmic work on the topic. While many recognize the necessity to incorporate explainability features in AI systems, how to address real-world user needs for understanding AI remains an open question. By interviewing 20 UX and design practitioners working on various AI products, we seek to identify gaps between the current XAI algorithmic work and practices to create explainable AI products. To do so, we develop an algorithm-informed XAI question bank in which user needs for explainability are represented as prototypical questions users might ask about the AI, and use it as a study probe. Our work contributes insights into the design space of XAI, informs efforts to support design practices in this space, and identifies opportunities for future XAI work. We also provide an extended XAI question bank and discuss how it can be used for creating user-centered XAI.},
   author = {Q. Vera Liao and Daniel Gruen and Sarah Miller},
   doi = {10.1145/3313831.3376590},
   isbn = {9781450367080},
   journal = {Conference on Human Factors in Computing Systems - Proceedings},
   keywords = {explainable AI,human-AI interaction,user experience},
   month = {4},
   publisher = {Association for Computing Machinery},
   title = {Questioning the AI: Informing Design Practices for Explainable AI User Experiences},
   year = {2020},
}
@article{Carvalho2019,
   abstract = {Machine learning systems are becoming increasingly ubiquitous. These systems&rsquo;s adoption has been expanding, accelerating the shift towards a more algorithmic society, meaning that algorithmically informed decisions have greater potential for significant social impact. However, most of these accurate decision support systems remain complex black boxes, meaning their internal logic and inner workings are hidden to the user and even experts cannot fully understand the rationale behind their predictions. Moreover, new regulations and highly regulated domains have made the audit and verifiability of decisions mandatory, increasing the demand for the ability to question, understand, and trust machine learning systems, for which interpretability is indispensable. The research community has recognized this interpretability problem and focused on developing both interpretable models and explanation methods over the past few years. However, the emergence of these methods shows there is no consensus on how to assess the explanation quality. Which are the most suitable metrics to assess the quality of an explanation? The aim of this article is to provide a review of the current state of the research field on machine learning interpretability while focusing on the societal impact and on the developed methods and metrics. Furthermore, a complete literature review is presented in order to identify future directions of work on this field.},
   author = {Diogo V. Carvalho and Eduardo M. Pereira and Jaime S. Cardoso},
   doi = {10.3390/ELECTRONICS8080832},
   issn = {2079-9292},
   issue = {8},
   journal = {Electronics 2019, Vol. 8, Page 832},
   keywords = {XAI,explainability,interpretability,machine learning},
   month = {7},
   pages = {832},
   publisher = {Multidisciplinary Digital Publishing Institute},
   title = {Machine Learning Interpretability: A Survey on Methods and Metrics},
   volume = {8},
   url = {https://www.mdpi.com/2079-9292/8/8/832/htm https://www.mdpi.com/2079-9292/8/8/832},
   year = {2019},
}
@article{Ongena2021,
   abstract = {Objective: To investigate the general population's view on the use of artificial intelligence (AI) for the diagnostic interpretation of screening mammograms. Methods: Dutch women aged 16 to 75 years were surveyed using the Longitudinal Internet Studies for the Social sciences panel, representative for the Dutch population. Attitude toward AI in mammography screening was measured by means of five items: necessity of a human check; AI as a selector for second reading; AI as a second reader; developer is responsible for error; and radiologist is responsible for error. Results: Of the 922 participants included. Discussion: Despite recent breakthroughs in the diagnostic performance of AI algorithms for the interpretation of screening mammograms, the general population currently does not support a fully independent use of such systems without involving a radiologist. The combination of a radiologist as a first reader and an AI system as a second reader in a breast cancer screening program finds most support at present. Accountability in case of AI-related diagnostic errors in screening mammography is still an unresolved conundrum.},
   author = {Yfke P. Ongena and Derya Yakar and Marieke Haan and Thomas C. Kwee},
   doi = {10.1016/J.JACR.2020.09.042},
   issn = {1546-1440},
   issue = {1},
   journal = {Journal of the American College of Radiology},
   keywords = {Artificial intelligence,breast cancer,mammography,mass screening,surveys and questionnaires},
   month = {1},
   pages = {79-86},
   pmid = {33058789},
   publisher = {Elsevier},
   title = {Artificial Intelligence in Screening Mammography: A Population Survey of Women’s Preferences},
   volume = {18},
   year = {2021},
}
@article{York2020,
   author = {Thomas York and Heloise Jenney and Gareth Jones},
   doi = {10.1136/BMJHCI-2020-100233},
   issn = {2632-1009},
   issue = {3},
   journal = {BMJ Health & Care Informatics},
   keywords = {BMJ Health Informatics,computer methodologies,health care,information science,patient care},
   month = {11},
   pages = {e100233},
   pmid = {33187956},
   publisher = {BMJ Publishing Group Ltd},
   title = {Clinician and computer: a study on patient perceptions of artificial intelligence in skeletal radiography},
   volume = {27},
   url = {https://informatics.bmj.com/content/27/3/e100233 https://informatics.bmj.com/content/27/3/e100233.abstract},
   year = {2020},
}
@article{Ongena2020,
   author = {Yfke P. Ongena and Marieke Haan and Derya Yakar and Thomas C. Kwee},
   doi = {10.1007/S00330-019-06486-0/TABLES/4},
   issn = {14321084},
   issue = {2},
   journal = {European Radiology},
   keywords = {Artificial intelligence,Patients,Radiology,Surveys and questionnaires},
   month = {2},
   pages = {1033-1040},
   pmid = {31705254},
   publisher = {Springer},
   title = {Patients’ views on the implementation of artificial intelligence in radiology: development and validation of a standardized questionnaire},
   volume = {30},
   url = {https://link.springer.com/article/10.1007/s00330-019-06486-0},
   year = {2020},
}
@article{Richardson2021,
   abstract = {While there is significant enthusiasm in the medical community about the use of artificial intelligence (AI) technologies in healthcare, few research studies have sought to assess patient perspectives on these technologies. We conducted 15 focus groups examining patient views of diverse applications of AI in healthcare. Our results indicate that patients have multiple concerns, including concerns related to the safety of AI, threats to patient choice, potential increases in healthcare costs, data-source bias, and data security. We also found that patient acceptance of AI is contingent on mitigating these possible harms. Our results highlight an array of patient concerns that may limit enthusiasm for applications of AI in healthcare. Proactively addressing these concerns is critical for the flourishing of ethical innovation and ensuring the long-term success of AI applications in healthcare.},
   author = {Jordan P. Richardson and Cambray Smith and Susan Curtis and Sara Watson and Xuan Zhu and Barbara Barry and Richard R. Sharp},
   doi = {10.1038/s41746-021-00509-1},
   issn = {2398-6352},
   issue = {1},
   journal = {npj Digital Medicine 2021 4:1},
   keywords = {Health policy,Medical ethics,Translational research},
   month = {9},
   pages = {1-6},
   publisher = {Nature Publishing Group},
   title = {Patient apprehensions about the use of artificial intelligence in healthcare},
   volume = {4},
   url = {https://www.nature.com/articles/s41746-021-00509-1},
   year = {2021},
}
@article{Amershi2019,
   abstract = {Advances in artifcial intelligence (AI) frame opportunities and challenges for user interface design. Principles for human-AI interaction have been discussed in the human-computer interaction community for over two decades, but more study and innovation are needed in light of advances in AI and the growing uses of AI technologies in human-facing applications. We propose 18 generally applicable design guidelines for human-AI interaction. These guidelines are validated through multiple rounds of evaluation including a user study with 49 design practitioners who tested the guidelines against 20 popular AI-infused products. The results verify the relevance of the guidelines over a spectrum of interaction scenarios and reveal gaps in our knowledge, highlighting opportunities for further research. Based on the evaluations, we believe the set of design guidelines can serve as a resource to practitioners working on the design of applications and features that harness AI technologies, and to researchers interested in the further development of guidelines for human-AI interaction design.},
   author = {Saleema Amershi and Dan Weld and Mihaela Vorvoreanu and Adam Fourney and Besmira Nushi and Penny Collisson and Jina Suh and Shamsi Iqbal and Paul N. Bennett and Kori Inkpen and Jaime Teevan and Ruth Kikin-Gil and Eric Horvitz},
   doi = {10.1145/3290605.3300233},
   isbn = {9781450359702},
   journal = {Conference on Human Factors in Computing Systems - Proceedings},
   keywords = {AI-infused systems,Design guidelines,Human-AI interaction},
   month = {5},
   publisher = {Association for Computing Machinery},
   title = {Guidelines for human-AI interaction},
   year = {2019},
}
@inproceedings{Yin2019,
   abstract = {We address a relatively under-explored aspect of human–computer interaction: people’s abilities to understand the relationship between a machine learning model’s stated performance on held-out data and its expected performance post deployment. We conduct large-scale, randomized human-subject experiments to examine whether laypeople’s trust in a model, measured in terms of both the frequency with which they revise their predictions to match those of the model and their self-reported levels of trust in the model, varies depending on the model’s stated accuracy on held-out data and on its observed accuracy in practice. We find that people’s trust in a model is affected by both its stated accuracy and its observed accuracy, and that the effect of stated accuracy can change depending on the observed accuracy. Our work relates to recent research on interpretable machine learning, but moves beyond the typical focus on model internals, exploring a different component of the machine learning pipeline.},
   author = {Ming Yin and Jennifer Wortman Vaughan and Hanna Wallach},
   doi = {10.1145/3290605.3300509},
   isbn = {9781450359702},
   journal = {Conference on Human Factors in Computing Systems - Proceedings},
   keywords = {Human-subject experiments,Machine learning,Trust},
   month = {5},
   publisher = {Association for Computing Machinery},
   title = {Understanding the effect of accuracy on trust in machine learning models},
   year = {2019},
}
@article{Weld2019,
   author = {Daniel S. Weld and Gagan Bansal},
   doi = {10.1145/3282486},
   issn = {15577317},
   issue = {6},
   journal = {Communications of the ACM},
   pages = {70-79},
   publisher = {Association for Computing Machinery},
   title = {The challenge of crafting intelligible intelligence},
   volume = {62},
   year = {2019},
}